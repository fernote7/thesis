# The Heston Model Implementation

```{r results, echo=FALSE, message=FALSE, results='asis', eval=FALSE}
source("/home/fteixeira/Documents/thesis/r_code/results.r")
```

In section \@ref(hes1) we presented Heston's SDE system in one of its structures. Another common way [@broadie2006exact; @andersen; @kahl2006fast] to write down the system is using the property presented in subsection \@ref(corr) as in equation \@ref(eq:heston2).   

\begin{align}
\label{eq:heston2}
\begin{split}
dS_t &= \mu S_t dt + \rho \sqrt{V_t} dB_t + \sqrt{1 - \rho^2} \sqrt{V_t} S_t dW_t \\
dV_t &= k(\theta - V_t)dt + \sigma \sqrt{V_t} dB_t 
\end{split}
\end{align}




## Characteristic Function

The Heston model characteristic function is firstly presented in the 1993 Steven Heston's paper [@heston1993closed] and is described below [@dunn2014estimating]:

\begin{align}
f(S_t, V_t, t) = e^{A(T-t)+B(T-t)S_t + C(T-t)V_t + i \phi S_t}
\end{align}

If we let $\tau = T-t$, then the explicit form of the Heston characteristic function is:

\begin{align*}
f(i \phi) &= e^{A(\tau)+B(\tau)S_t + C(\tau)V_t + i \phi S_t} \\
A(\tau) &= r i \phi \tau + \frac{\kappa \theta}{\sigma^2} \left[ - (\rho \sigma i \phi - \kappa - M) \tau - 2 \ln\left(\frac{1-N e^{M \tau}}{1-N}\right) \right] \\
B(\tau) &= 0 \\
C(\tau) &= \frac{(e^{M \tau}-1)(\rho \sigma i \phi - \kappa - M)}{\sigma^2 (1-N e^{M \tau})} \\
\text{Where:} & \\
M &= \sqrt{(\rho \sigma i \phi - \kappa)^2 + \sigma^2 (i \phi + \phi^2)} \\
N &= \frac{\rho \sigma i \phi - \kappa - M}{\rho \sigma i \phi - \kappa + M} \\
\end{align*}

This function is the driving force behind the following formula, that calculates the fair valur of a European call option at time $t$, given a strike price $K$, that expires at time $T$ [@dunn2014estimating]:

\begin{align} 
\label{eq:cfheston}
\begin{split}
C = & \frac{1}{2} S(t) + \frac{e^{-r(T-t)}}{\pi}\int_{0}^{\infty}{\Re \left[ \frac{K^{-i \phi} f(i \phi + 1)}{i \phi} \right] d\phi} \\
& -Ke^{-r(T-t)}\left( \frac{1}{2} + \frac{1}{\pi} \int_{0}^{\infty}{\Re \left[ \frac{K^{-i \phi} f(i \phi)}{i \phi} \right]}  d\phi \right)
\end{split}
\end{align}




## Euler Scheme - Full Truncation

We present here the Euler Scheme - Full Truncation algorithm [@broadie2006exact, dunn2014estimating, andersen] along with some insights on how it was implemented in R. The Euler discretization brings approximation paths to stock prices and variance processes. If we set $t_0 = 0 < t_1 < \dots < t_M = T$ as partitions of a time interval of $M$ equal segments of lenght $\delta t$, we have the following discretization for the stock price:

\begin{align}
S_{t+1} = S_t + rS_t + \sqrt{V_t} S_t Z_s
\end{align}

\noindent
And for the variance process:

\begin{align}
V_{t+1} = V_t + \kappa (\theta - V_t) + \sigma \sqrt{V_t} Z_v
\end{align}

\noindent
$Z_s$ being a standard normal random variable, i.e. $N\sim(0,1)$, we set $Z_t$ and $Z_v$ as two independent standard normal random variables and $Z_s$ and $Z_v$ having correlation $\rho$. This means we can write $Z_s = \rho Z_v + \sqrt{1-\rho^2} Z_t$

The immediate observable problem in the proposed discretization scheme is that $V$ can become negative with non-zero probability making the computation of $\sqrt{V_t}$ impossible [@andersen]. There are several proposed fixes that can be used, we chose the Full-Truncation (FT) and rewrite the equations as follows:


\begin{align}
S_{t+1} &= S_t + rS_t + \sqrt{V_{t}^{+}} S_t Z_s \\
V_{t+1} &= V_t + \kappa (\theta - V_{t}^{+}) + \sigma \sqrt{V_{t}^{+}} Z_v
\end{align}

\noindent
Where we use the notation $V_{t}^{+} = \max(V_{t}, 0)$. 

```{r calle, eval=FALSE, echo=FALSE}
hestoneuler <- function(S, X, r, v, theta, rho, k, sigma, t = 0, 
                        dt = NULL, tau = 1, N, sensibility = 15){

    set.seed(1)

    if(is.null(dt)){ dt <- (tau-t)/1000}

    ST <- NULL
    aux <- NULL
    sqrt_dt <- sqrt(dt)

    for(i in seq(t,tau,dt)){
        # browser()
        Zv <- rnorm(N)
        Zt <- rnorm(N)
        Zs <- rho * Zv + (sqrt(1 - (rho^2)) * Zt)

        # v[v <= 0] = 0

        aux <- v
        aux[v < 0] <- 0
        sqrt_aux <- sqrt(aux)
        S <- S * (1 + r * dt + sqrt_aux * Zs * sqrt_dt)
        S[S <= 0] = 0

        v <- v + k * dt * (theta - aux) + 
          sigma * sqrt_aux * Zv * sqrt_dt


        ST <- rbind(ST,S)
    }

    rm(aux, v, Zv, Zt, Zs, S)

    ST <- as.matrix(ST, ncol=N)
    # matplot(graf, type = 'l')
    # abline(h=X, lwd=2, col='red')
    media <- c()

    for (j in (X-sensibility):(X+sensibility)){

        Result <- ST[nrow(ST),] - j
        Result[Result <= 0] <- 0
        media <-c(media, mean(Result))

    }

    media <- media * exp(-r*tau)
    # plot(media, ylab = "Call price", xlab = "Strike price")


    Result <- ST[nrow(ST),] - X
    Result[Result <= 0] = 0
    call = mean(exp(-r*tau)*Result)

    lista = list('call' = call, 'Result' = Result, 
                 'Spot' = ST, 'sensib' = media)
    return(lista)
}
```


## Kahl-Jackel

Kahl-Jackel propose a discretization method they refer to as the ``IJK'' method  [@andersen; @kahl2006fast] that coupled with the implicit Milstein scheme for the variance lands the system of equations \@ref(eq:kj1) and \@ref(eq:kj2). It is possible to verify that this discretization always results in positive paths for $V$ if $4 \kappa \theta > \sigma^2$. Unfortunately, this inequality is rarely satisfied when we plug real market data to calibrate the parameters.

\begin{small}
\begin{align}
\label{eq:kj1}
\ln \hat{S}(t + \Delta) &= \ln \hat{S}(t) - \frac{\Delta}{4}\left( \hat{V}(t+\Delta) + \hat{V}(t) \right) + \rho \sqrt{\hat{V}(t)}Z_v\sqrt{\Delta} \\ \nonumber
&+ \frac{1}{2} \left( \sqrt{\hat{V}(t+\Delta)} + \sqrt{\hat{V}(t)} \right) \left( Z_S \sqrt{\Delta} - \rho Z_V \sqrt{\Delta}\right) + \frac{1}{4} \sigma \rho \Delta \left( Z_{V}^{2} - 1 \right) \\
\label{eq:kj2}
\hat{V}(t+\Delta) &= \frac{\hat{V}(t) + \kappa \theta \Delta + \sigma \sqrt{\hat{V}(t)}Z_V \sqrt{\Delta}+ \frac{1}{4}\sigma^2 \Delta \left(Z_V^2-1 \right)}{1+ \kappa \Delta}
\end{align}
\end{small}

```{r callkj, eval=FALSE, echo=FALSE}
Hestoncallkj <- function(S, X, r, q, v, theta, rho, k, sigma, 
                         t = 0, dt = NULL, tau = 1, N, 
                         sensibility = 15){

    if(is.null(dt)){ dt <- (T-t)/1000}

    v <- rep(v,N)
    theta<- rep(theta,N)
    graf <- NULL
    S <- log(S)


    for(i in seq(t,tau,dt)){
     # browser()
    Zv <- rnorm(N)
    Zt <- rnorm(N)
    Zs <- rho * Zv + sqrt(1 - rho^2) * Zt


    vt <- (v + k * theta * dt + sigma * sqrt(v) * Zv * sqrt(dt) +
              (1/4) * sigma^2 * dt * ((Zv)^2 - 1))/(1 + k * dt)

    # print(vt)

    # if(!length(vt[vt <= 0]) == 0 & length(vt[vt <= 0]) >= 1){
    #     # browser()
    vt[vt <= 0] <- v[vt <= 0] + k * dt * (theta[vt <= 0] - 
                    pmax(v[vt <= 0],0)) + sigma * 
                    sqrt(pmax(v[vt <= 0],0)) * Zv[vt <= 0] * sqrt(dt)
    v <- vt
    v[v<=0] <- 0

    # }

    S <- S + (r - (v+vt)/4) * dt + rho * sqrt(pmax(v,0)) * 
          Zv * sqrt(dt) +
          (1/2) * (sqrt(pmax(v,0)) + sqrt(pmax(vt,0))) * 
          (Zs + rho * Zv) * 
          sqrt(dt) +
          ((rho * sigma * dt)/2) * ((Zv)^2 - 1)

    S[S <= 0] = 0
    graf <- rbind(graf,S)


    }

     # browser()

    graf <- as.matrix(graf, ncol=N)
    ST <- graf
    # matplot(graf, type = 'l')
    # abline(h=log(X), lwd=2, col='red')

    media <- c()

    # browser()

    for (j in (X-sensibility):(X+sensibility)){

        Result <- exp(ST[nrow(ST),]) - j
        Result[Result <= 0] <- 0
        media <-c(media, mean(Result))

    }

    media <- media * exp(-r*tau)


    # browser()


    Result <- exp(ST[nrow(ST),]) - X
    Result[Result <= 0] = 0
    call = mean(exp(-r*tau)*Result)

    lista = list('call' = call, 'Result' = Result, 
                 'Spot' = graf, 'sensib' = media)
    return(lista)

}
```



## Exact Algorithm 

In 2006, Broadie-Kaya [@broadie2006exact] propose a method that has a faster convergence rate, $\mathcal{O} \left( s^{-1/2}  \right)$ than some of the more famous schemes, such as Euler's and Milstein's discretizations. They build their idea to generate an exact sample from the distribution of the terminal stock price based on numerous papers [@heston1993closed, @scott1996, @willard1997, @romano1997]. The stock price and variance are as follows:

\begin{align} \label{eq:ea1}
S_t = S_0 \, exp \left[ \mu t - \frac{1}{2} \int_{0}^{t}{V_s ds} + \rho  \int_{0}^{t}{\sqrt{V_s d B_s}} + \sqrt{1 - \rho^2} \int_{0}^{t}{\sqrt{V_s} dW_s}\right]
\end{align}

The squared volatility of the variance process is:

\begin{align} \label{eq:ea2}
V_t = V_0 + \kappa \theta t - \kappa \int_{0}^{t}{V_s ds} + \sigma \int_{0}^{t}{\sqrt{V_s dB_s}}
\end{align}


The algorithm used to generate the model consists in four steps as follows:
 
\begin{itemize}
\item [\textit{Step} 1.] Generate a sample of $V_t$ given $V_0$
\item [\textit{Step} 2.] Generate a sample of $\int_0^t V_sds$ given $V_t$, $V_0$
\item [\textit{Step} 3.] Compute $\int_0^t \sqrt{V_s}dB_s$ given $V_t$, $V_0$ and $\int_0^t V_sds$
\item [\textit{Step} 4.] Generate a sample from the probability distribution of $S_t$, given $\int_0^t \sqrt{V_s}dB_s$ and $\int_0^t V_sds$
\end{itemize}




### Generate a sample of $V_t$ given $V_0$
The distribution of $V_t$ given $V_0$ for $0 < t$ is a noncentral chi-squared distribution [@cox1985theory; @baldeaux]:

$$V_t = \frac{\sigma^2 (1-e^{- \kappa t})}{4 \kappa} \mathcal{X}_{\delta}^{2} \left( \frac{4 \kappa e^{- \kappa t}}{\sigma^2 (1- e^{- \kappa t})} \times V_0\right)$$

where $\delta = \frac{4 \theta \kappa}{\sigma^2}$ and $\mathcal{X}_{\delta}^{2}(\lambda)$ denotes a noncentral chi-squared random variable with $\delta$ degrees of freedom and $\lambda$ as its noncentrality parameter.

Broadie and Kaya [-@broadie2006exact] sample generating Poisson and gamma distributions as in Johnson et al. [-@johnson1995]. We used the built-in function in R [@rlang] which uses this exact method for sampling.


### Generate a sample of $\int_0^t V_sds$ given $V_t$, $V_0$

After generating $V_t$, we follow the instructions in [-@broadie2006exact; -@johnson1995]. We use the characteristic function \@ref(eq:phi) to compute the probability density function $F(x)$.

\begin{align} \label{eq:phi}
\begin{split}
\Phi(a) &= \mathbb{E}\left[ exp \left( ia \int_{0}^{t}{V_s ds} \mid V_0,V_t \right)  \right] \\[10pt]
&= \frac{\gamma(a)e^{(-1/2)(\gamma(a)- \kappa) t} (1 - e^{- \kappa t})}{\kappa (1 - e^{- \gamma(a) t})} \\[10pt]
&\times exp \left\{\frac{V_0 + V_t}{\sigma^2} \left[ \frac{\kappa (1 + e^{- \kappa t})}{1 - e^{- \kappa t}} - \frac{\gamma(a) (1 + e^{- \gamma(a) t})}{1 - e^{- \gamma(a) t}} \right] \right\} \\[10pt]
&\times \frac{I_{0.5\delta - 1} \left[ \sqrt{V_0 V_t} \frac{4 \gamma(a) e^{-0.5 \gamma(a) t}}{\sigma^2 (1 - e^{- \gamma(a) t})} \right]}{I_{0.5\delta - 1}  \left[ \sqrt{V_0 V_t} \frac{4 \kappa e^{-0.5 \kappa t}}{\sigma^2 (1 - e^{- \kappa t})} \right]}
\end{split}
\end{align}

where $\gamma(a) = \sqrt{\kappa^2 - 2 \sigma^2 i a}$, $\delta$ was previously defined and $I_v(x)$ is the modified Bessel function of the first kind.

The probability distribution function is obtained in [@broadie2006exact; @baldeaux] by Fourier inversions using Feller [-@feller1971introduction]. We use the approach in Gil-Pelaez [-@gil1951note], equation \@ref(eq:fourier). We define $V(u,t)$ the random variable with the same distribution as the integral $\int_{u}^{t}{V_s ds}$, conditional on $V_u$ and $V_t$:

\begin{align} \label{eq:fourier}
F(x) \equiv Pr \left\{ V(u,t) \leq x \right\} = F_{X}(x)={\frac {1}{2}}-{\frac {1}{\pi }}\int _{0}^{\infty }{\frac {\operatorname {Im} [e^{-iux} phi (u)]}{u}}\,du
\end{align}

$\operatorname {Im}$ denotes the imaginary part of $e^{-iux} phi (u)$. Equation \@ref(eq:fourier) is computed numerically and we then sample it by inversion. 

Furthermore, we also introduce a simpler version for this step, that computes this integral approximation, using the solution $\int_{u}^{t}{V_s ds} = \frac{1}{2} \left( V_u + V_t  \right)$



### Compute $\int_0^t \sqrt{V_s}dB_s$ given $V_t$, $V_0$ and $\int_0^t V_sds$

From equation \@ref(eq:ea2) we are now able to compute this integral.


\begin{align} \label{eq:ea3}
\int_{0}^{t}{\sqrt{V_s dB_s}} = \frac{V_t - V_0 - \kappa \theta t + \kappa \int_{0}^{t}{V_s ds}}{\sigma} 
\end{align}


The last step of the algorithm consists of computing the conditional distribution of $log S_t$ based on the fact that the process for $V_t$ is independent from $dB_t$, and the distribution of $\int_0^t{\sqrt{V_s} dB_s}$ is normal with mean $0$ and variance $\int_0^t{V_s ds}$, given $V_t$.


$$m(u,t) = \log S_0 + \left[ \mu t - \frac{1}{2} \int_{0}^{t}{V_s ds} + \rho  \int_{0}^{t}{\sqrt{V_s d B_s}} + \sqrt{1 - \rho^2} \int_{0}^{t}{\sqrt{V_s} dW_s}\right]$$

and variance

$$\sigma^2(0,t) = \left( 1 - \rho^2 \right) \int_0^t{V_s ds}$$


We generate the $S_t$ sample using a standard normal random variable $Z$ and set:

$$S_t = e^{m(0,t) + \sigma (0,t) Z}$$





```{r callea, eval=FALSE, echo=FALSE}
hestonea <- function(S, X, r, v, theta, rho, k, sigma, t = 0, 
                     dt = NULL, tau = 1, N, sensibility = 15){

    if(is.null(dt)){ dt <- (tau-t)/1000}

    ST <- NULL

    d <- (4 * k * theta)/(sigma)^2
    c0 <- (sigma^2 * (1 - exp(-k*dt)))/(4*k)

    # browser()

    for(i in seq(t,tau,dt)){

        # sampling V
        lambda <- (4*k*exp(-k*dt)*v)/(sigma^2 * (1-exp(-k*dt)))
        vt <- c0 * rchisq(n = N, df = d, ncp = lambda)

        # Sampling int{V}
        int_v <- dt * ((1/2) * v + (1/2) * vt)

        # Sampling int{v}dw
        int_vdw <- (1/sigma) * (vt - v - k * theta * dt + k  * int_v)

        # Sampling S
        m <- log(S) + (r * dt - (1/2) * int_v + rho * int_vdw)
        std <- sqrt((1 - rho^2)) * sqrt(int_v)
        S <- exp(m + std * rnorm(N))
        v <- vt
        ST <- rbind(ST,S)
    }

    ST <- as.matrix(ST, ncol=N)
    # matplot(graf, type = 'l')
    # abline(h=X, lwd=2, col='red')
    media <- c()

    for (j in (X-sensibility):(X+sensibility)){

        Result <- ST[nrow(ST),] - j
        Result[Result <= 0] <- 0
        media <-c(media, mean(Result))

    }

    media <- media * exp(-r*tau)
    # plot(media, ylab = "Call price", xlab = "Strike price")


    Result <- ST[nrow(ST),] - X
    Result[Result <= 0] = 0
    call = mean(exp(-r*tau)*Result)

    lista = list('call' = call, 'Result' = Result, 
                 'Spot' = ST, 'sensib' = media)
    return(lista)
}
```









<!-- Required to number equations in HTML files -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>


