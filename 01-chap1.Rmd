---
output:
  pdf_document: default
  html_document: default
---


```{r include_packages, include = FALSE}
# This chunk ensures that the thesisdown package is
# installed and loaded. This thesisdown package includes
# the template files for the thesis and also two functions
# used for labeling and referencing
if(!require(plotrix))
  install.packages("plotrix", repos = "http://cran.rstudio.com")
if(!require(Sim.DiffProc)) 
  install.packages("Sim.DiffProc", repos = "http://cran.rstudio.com")
```
<!--
This is for including Chapter 1.  Notice that it's also good practice to name your chunk.  This will help you debug potential issues as you knit.  The chunk above is called intro and the one below is called chapter1.  Feel free to change the name of the Rmd file as you wish, but don't forget to change it here from chap1.Rmd.
-->

<!--
The {#rmd-basics} text after the chapter declaration will allow us to link throughout the document back to the beginning of Chapter 1.  These labels will automatically be generated (if not specified) by changing the spaces to hyphens and capital letters to lowercase.  Look for the reference to this label at the beginning of Chapter 2.
-->


# Theoretical Framework {#lt-review}

This chapter presents the concepts of stochastic calculus, from the historic conception of how it first arose through the basic principles and applications in finance. We address with more care the classical Black-Scholes model and its limitations and the Heston model. This model is also well known, it brings the concept of stochastic volatility in it, which presents results closer to reality.


[//]: is a generalization of the Riemann-Stieltjes integral 


## Stochastic Calculus

Stochastic calculus arises from stochastic processes and allows the creation of a theory of integration where both the integrand and integrator terms are stochastic processes. Stochastic calculus was created by the Japanese mathematician Kiyosi Itô in the 1940s and 1950s and is used for modeling financial options and in another wide variety of fields [@ubbo]. In this chapter we present the historical contexts in which the tools and models are used, but our focus is introducing the concepts and notations that will be further used in our work.


### Stochastic differential equation - SDE


At first, before introducing stochastic differential equation, it is helpful to start with ordinary differential equation. Let $x_t$ denote an asset at time $t$ so that the change in the asset at time $t$ is given by the following deterministic differential equation:

\begin{align}
dx_t &= f(t, x_t)dt \\
x(0) &= x_0 \nonumber
\end{align}

We now add a ``noise'' to this equation:

\begin{align} \label{eq:sde1}
dx_t &= \overbrace{\underbrace{f(t, x_t)}_\text{drift}dt}^\text{deterministic} + \overbrace{\underbrace{g(t, x_t)}_\text{diffusion}dW_t}^\text{random shock} \\
x(0) &= x_0 \nonumber
\end{align}

This ``noise''  $W_t$ is a *random* Wiener process (which will be clarified below) and $x_0$ is our initial value. 

The $g(t, x_t)$ part of the SDE is often referred as *diffusion coefficient*. Before moving on, we must carefully define what the term *random* means and the best way to begin doing so is to precisely define a probability space:

```{definition, name="Probability Space"}
A triple ($\Omega$, $\mathcal {U}$, $\mathcal {P}$) is called a \textit{probability space} provided $\Omega$ is any set, $\mathcal {U}$ is a $\sigma$-algebra of subsets of $\Omega$ and $\mathcal {P}$ is a probability measure on $\mathcal {U}$ .
```

### Brownian Motion

The Brownian motion is the name given to the irregular motion observed in the motion of pollen particles suspended in fluid resulting from particle collision with atoms or molecules. It is named after Robert Brown, the first to have observed the movement in 1828. He noted two characteristic in the pollen movement [@ubbo]:

* the path of a given particle is very irregular, having a tangent at no point

* the motion of two distinct particles appear to be independent

The first quantitative works in Brownian motion come from an interest in stock price fluctuation by Bachelier in 1900. Albert Einstein also leaned over the subject and in 1905 derived the transition density for Brownian motion from molecular-kinetic theory of heat [@ubbo; @karatzas2012brownian].



In 1923, the Wiener process was coined in honor of Norbert Wiener mathematical proof of existence of the Brownian motion and stating its properties ^[More can be found on [@helgadottir2016option; @evans; @rosenthal].].


```{definition, name="Wiener Process"}
Given a probability space ($\Omega$, $\mathcal {U}$, $\mathcal {P}$), a stochastic process $W_t$ defined in this space is a *Wiener process* if it satisfies the following properties:

\begin{itemize}
  \item  $W_{0}=0$
  
  \item The change in $W$, given by $\Delta W = W_{t+\Delta}-W_{t}$, is normally distributed with mean zero and standard deviation $\sqrt{\Delta t}$, meaning that $\Delta W = \epsilon\sqrt{\Delta t}$, where $\epsilon$ is $N(0,1)$.
  
  \item If the increment $\Delta t_1$ does not overlap with the time increment $\Delta t_2$, then $\Delta W_1$ and $\Delta W_2$ are independent.
  
  \item The process is continuous, meaning that there are no jumps in the path, almost surely.

\end{itemize}
```




Furthermore, if a process is a Wiener process, we shall have the following results by construction:

\begin{itemize}  
  \item The process is Markovian. This means that the conditional expectation of $W_{t+\Delta t}$ given its entire history is equal to the conditional expectation of $W_{t+\Delta t}$ given today's information. This can be written as: $E[W_{t+\Delta t}|W_t]$.
  
  \item Consider the time interval $[0,t]$ with $n$ equally spaced intervals given by $t_i = \frac{it}{n}$. Then the paths of the Brownian motion have unbounded variation, they are not differentiable. The quadratic variation is given by $\sum_{i=1}^{n}{(Z_{t_i}-Z_{t_{i-1}})^2} \rightarrow t$, meaning that when $n$ increases it stays constant at $t$. 
\end{itemize}

<!-- \ref{wiener} -->
<!-- \@ref(fig:wiener) -->

```{r wiener, echo=FALSE, message=FALSE, results='asis', fig.asp = 0.7, fig.width = 5, fig.align='center', fig.cap='A Wiener process trajectory path example \\label{wiener}'}
source("./r_code/wiener.R")
```



#### Correlated Brownian Motions {#corr}

Two independent Brownian motions that are correlated can describe a new process $Z_t$. Let $W_1$ and $W_2$ be these two *independent* Brownian motions and let $-1 \leq \rho \leq 1$ be a given number. For $0 \leq t \leq T$ define the new process $Z_t$ as [@ubbo]:

\begin{align}
\label{eq:corr_brow}
Z_t = \rho W_{1,t} + \sqrt{1-\rho^2}W_{2,t}
\end{align}

\noindent
This equation is a linear combination of independent normals at each timestep $t$, so $Z_t$ is normally distributed. It is proven that $Z$ is a Brownian motion and that $Z$ and $W_{1,t}$ have correlation $\rho$ [@ubbo, pp. 514].





### Itô's Integral


Formally, the SDE  presented in equation \@ref(eq:sde1) is interpreted as [@tong2012option; @evans; @steele2012stochastic; @ito1951; @ito1962; @rosenthal]:

\begin{align} 
\label{eq:sde2}
x_t &= x_0 + \int_{0}^{t}{f(s, x_s)ds} + \int_{0}^{t}{g(s, x_s)dW_s}
\end{align}


\noindent
for some $f$, $g$ and $s \in [0,t]$.



#####################


The Itô integral can, as the Riemann integral, be approximated by a finite sum \footnote{There is another important stochastic integral, called the \textit{Stratonovich Integral} that unlike the Itô's integral, respects the conventional calculus chain rule. Also, the integral is evaluated at the interval's midpoint, instead of its left extreme. A Stratonovich integral can be expressed as an Itô integral and vice versa.}. Also, it has a definition as a certain limit. Itô's lemma \@ref(lem:itolemma) plays the same role as the fundamental theorem of calculus in allowing to evaluate integrals. It presents an extra term not encountered in the conventional calculus theorem that is due to the non-smoothness characteristics of Brownian motion paths. It is possible, though, to define the integral as the summation below: 

\begin{align}
Y_{\Delta t}(t) = \sum_{t_k < t}{g(t_k)\Delta W_k}
\end{align}

\noindent
with the usual notions $t_k = k\Delta t$, and $\Delta W_k = W(t_{k+1})-W(t_k)$. And in a more rigorous form, if the limit exists, then the Ito integral is:

\begin{align}
Y(t)  = \lim\limits_{\Delta t \to 0} Y_{\Delta t}(t)
\end{align}


It is essential that the *forward difference*, that is $W(t_{k+1})-W(t_k)$ is used rather than the backward difference $W(t_{k})-W(t_{k-1})$, which would be wrong.


```{lemma, name = "Itô's Lemma", label="itolemma"}

Assume that $S_t$ has a stochastic differential given by:

\begin{align}
dS_t = \mu_t dt + \sigma_t dW_t 
\end{align}

\noindent
for $\mu_t$, $\sigma_t$ and $t \in [0,T]$ and that $\mathbb{C}^{2,1} \left( \mathbb{R} \times \left[0, T \right]\right)$. Assume $u: \mathbb{R} \times [0, T] \rightarrow \mathbb{R}$ is continuous and that $\frac{\partial u}{\partial t}$, $\frac{\partial u}{\partial x}$, $\frac{\partial^2 u}{\partial x^2}$ exist and are continuous.

\begin{align*}
Y_t := u(S_t, t)
\end{align*}


\noindent
Then Y has the following stochastic differential:

\begin{align} 
\label{eq:ito}
\begin{split}
    dY_t &= \frac{\partial u}{\partial t}dt + \frac{\partial u}{\partial x} dS_t + \frac{1}{2}\frac{\partial^2 u}{\partial x^2}\sigma_t^2 dt  \\[10pt] 
    &= \left( \frac{\partial u}{\partial t} + \mu_t \frac{\partial u}{\partial x} + \frac{1}{2}\frac{\partial^2 u}{\partial x^2}\sigma_t^2 \right) dt + \sigma_t \frac{\partial u}{\partial x} dW_t
\end{split}
\end{align}

\noindent 
where the argument of $\frac{\partial u}{\partial t}$, $\frac{\partial u}{\partial x}$ and $\frac{\partial^2 u}{\partial x^2}$ above is $\left( S_t, t \right)$ .
```



Equation \@ref(eq:ito) is the stochastic equivalent to the chain rule, also known as Itô's formula or Itô's chain rule. The proof to this theorem is based on the Taylor expansion of the function $f(S_t, t)$ [@tong2012option; @evans]. For practical use you should write out a second-order Taylor expansion for the function to be analyzed and apply the  multiplication table [@ubbo] presented in Table \@ref(mat).

```{r box-calc, echo=FALSE, message=FALSE, results='asis'}
source("./r_code/box.r")
```



#### Itô's Integral Properties

Let $f$, $g$ $\in$ $\mathcal{V}$ and let $0 \leq t_0 < u < T$. Then

\begin{enumerate}[label=(\roman*)]
  \item $\displaystyle \int_{t_0}^{T}{f dB_t} = \int_{t_0}^{u}{f dB_t} + \int_{u}^{T}{f dB_t}$
  \item $\displaystyle \int_{t_0}^{T}{(\alpha f + \beta g) dB_t} = \alpha  \int_{t_0}^{T}{f dB_t} + \beta  \int_{t_0}^{T}{g dB_t}$
  \item $\displaystyle \mathbb{E}\left[ \int_{t_0}^{T}{fdB_t}\right] = 0 $
  \item $\displaystyle \mathbb {E} \left[\left(\int_{0}^{t}H_{s}\,dB_{s}\right)^{2}\right]=\mathbb {E} \left[\int _{0}^{t}H_{s}^{2}\,ds\right]$ (Isometry)
  \item $ \displaystyle \mathbb {E}\left[ \int_{t_0}^{T}{f dB_t \mid \mathcal{F}_{s}} \right] = \int_{t_0}^{s}{f dB_t}, \,\,\,\,\,\,\,\,\,\,\,\,\,\, for \,\, s < T.$  (Martingale\footnote{A martingale is a stochastic process with certain characteristics. The main one is that the expected value in time $t+\Delta t$ for $X$ is the $X$ value in $t$. This means there are no winning strategies when we are dealing with martingales (unlike when we play poker, for example). A Wiener process is a martingale.})
\end{enumerate}


 





## Black-Scholes Model

### Basics


The Black-Scholes (B-S) model arises from the need to price european options in the derivative markets. Derivatives are financial instruments traded in the market, stock exchange or over-the-counter (OTC) market, whose values depend on the values of an underlying asset. [@black1973pricing; @yang2013valuing; @salomao2011precificaccao]

* A call option is a derivative that gives its bearer the right, but not the obligation, to purchase a specific asset by a fixed price before or on a given date. 

* A put option is a derivative that gives its bearer the right, but not the obligation, to sell a specific asset by a fixed price before or on a given date.

The trading price of the option is called the option *premium* and the asset from which the option derives is called the *underlying asset*. This asset may be interest rates, exchange rates, stock exchanges indices, commodities or stocks. The fixed price in contract in which the underlying asset might to be bought or sold is the *strick price*. The option expiration date is called the *maturity*. [@salomao2011precificaccao; @black1973pricing]

There are two major different option types: European and American. The difference between these two is that the bearer of the first may exercise it only at the end of its life, at its maturity while the latter can be exercised at any given time until its maturity. [@black1973pricing; @merton1973theory]


<!-- \begin{definition}{(Implicit volatility)}  -->
<!-- Given all the option's parameters in a precification model and its market price, the option's volatility is called the \textit{implicit volatility}. -->
<!-- \end{definition} -->



#### Geometric Brownian Motion {#gbm}

A stochastic process $S_t$ is a Geometric Brownian Motion ^[There is an Arithmetic Brownian Motion: $dS_t = \mu dt + \sigma dB_t$. More information can be obtained in [@ubbo].] if it is described by the solution of the following stochastic differential equation [@ubbo; @tong2012option; @tsay2005analysis].

\begin{align}
dS_t = \mu S_t dt + \sigma S_t dW_t
\end{align}

\noindent
for given constants $\mu \in {\rm I\!R}$ and $\sigma > 0$. Also, the assumed initial value is positive, $S_0 >0$.


Figure \@ref(fig:gbm) shows the GBM ^[Also known as exponential Brownian motion.], which is quite often applied to model the dynamics of some financial assets because of its properties [@iacus2009simulation]. Equation \@ref(eq:sde3) shows the formula to generate a GBM and we provide proof of this solution in appendix \@ref(bsformula) ^[An intuitive proof can be found at [@krouglov2006intuitive].] 


\begin{align} \label{eq:sde3}
S_t = S_0 \times exp{\left( \left( \mu - \frac{\sigma^2}{2} \right) t + \sigma W_t \right)}, \;\; t > 0
\end{align}


```{r gbm, echo=FALSE, message=FALSE, results='asis', fig.asp = 0.7, fig.width = 5, fig.align='center', fig.cap='A GBM trajectory path example \\label{gbm}'}
source("./r_code/gbm.R")
```



The Black-Scholes model provides analytical solution to the price of a European call at time $t$  and can be described as follows [@yang2013valuing; @black1973pricing; @helgadottir2016option]:

\begin{align}
C(S_{t},t)&=N(d_{1})S_{t}-N(d_{2})Ke^{-r(T-t)}\\[10pt]
d_{1}&={\frac {1}{\sigma {\sqrt {T-t}}}}\left[\ln \left({\frac {S_{t}}{K}}\right)+\left(r+{\frac {\sigma ^{2}}{2}}\right)(T-t)\right]\\[10pt]
d_{2}&=d_{1}-\sigma {\sqrt {T-t}}
\end{align}




\noindent
Where:

* $S_{t}$ is the spot price of the underlying asset at time $t$
* $r$ is the risk free rate (generally an annual rate)\footnote{Assumed to be constant. \label{teste}}
* $\sigma$ is the volatility of returns of the underlying asset \footnote{See footnote 1.}
* $N(\cdot )$ is the cumulative distribution function of the standard Gaussian distribution
* $K$ is the strike price
* $T-t$ is the time to maturity 


\noindent
Also, the stock price path is a Geometric Brownian Motion as previously stated, and is under the risk-neutral measure with the following dynamics [@helgadottir2016option; @nmof]: 

\begin{align}
dS_{t} = (r-q)S_td_t+\sigma S_t dW_t
\end{align}

\noindent
Here $W_t$ is a Wiener process [@black1973pricing; @nmof], $r$ is the risk free rate and $q$ is the dividend yield\footnote{$r$ and $q$ are assumed to be constant.} and $t$ denotes the current point in time. 





### Limitations


Although the Black-Scholes is very popular and the *de facto* standard in the market there are implications to the B-S model assumptions that affect the results and that are unrealistic. The main assumption that does not hold up is the deterministic (constant) volatility, that can more accurately be described as a stochastic process since we observe that small moves usually are followed by small moves and large moves by large moves.   [@yang2013valuing; @helgadottir2016option]

Other assumptions that are critical to the B-S model and are not always observed in practice refer to the asset's continuity through time (no jumps), being allowed to perform continuous hedge without transactions costs and normal (Gaussian) returns.

Most models focus on the volatility problem because transaction costs often implies rises in volatility and that fat-tails (abnormal) returns can be simulated by stochastic volatility and market or volatility jumps.

<!-- Out of all the parameters that the call price depends on in the Black Scholes -->
<!-- model, the volatility is the only one that is unobservable, but we can let the -->
<!-- market of liquid derivatives decide which volatility has to be plugged in the -->
<!-- model. The calibrated σ* is the one that fits best the model prices to the observed -->
<!-- market prices, by minimizing the sum of square errors: -->
<!-- σ ∗ = arg q 	min -->
<!-- 2,o -->
<!-- C mn S " , K 2 , σ, T o − C p S", K 2 , T o -->
<!-- . -->
<!-- Knowing S, K, T, r, q and the quoted option call value it is possible to extract the -->
<!-- implied volatility. -->




<!-- [@tong2012option] -->
<!-- Using Black-Scholes option pricing model, the price of a call option is the function of -->
<!-- the spot (current) price S(0), interest rate r, the strike K, the constant volatility σ -->
<!-- and the maturity T . Except for the volatility σ, all the other variables are observable. -->
<!-- Since the quoted option price C obs is observable, using the Black-Scholes formula we -->
<!-- can therefore calculate or imply the volatility that is consistent with the quoted his- -->
<!-- torical option prices and observed variables. We can therefore define implied volatility -->
<!-- σimpl by -->
<!-- C BS (0; r, K, T, σimpl , S(0)) = C obs -->
<!-- where C BS is the option price calculated by the Black-Scholes formula (equation -->
<!-- (3.3.9)). Implied volatility surfaces are graphs plotting σimpl for each call option’s -->
<!-- strike K and expiration T . Theoretically, options whose underlying is governed by -->
<!-- the geometric Brownian motion should have a flat implied volatility surface, since -->
<!-- volatility is a constant. -->


<!-- [@yang2013valuing] -->
<!-- Volatility is a measure for variation of price of a stock over time. Stochastic volatility is described as -->
<!-- processes in which the return variation dynamics include an unobservable shock that cannot be predicted -->
<!-- using current available information. Stochastic volatility models, which let the volatility follow Brownian -->
<!-- motion, make the option price much better adapted to the realities of the market. -->


## Stochastic Volatility models

Introducing stochastic volatility to models brings complexity, but enables modeling some features observed in reality that are crucial, like the randomic market volatility effects, skewness (market returns are more realistically modeled) and volatility smile ^[The name derives from the concave shape of the graph, which resembles a smile.] (see Figure \@ref(fig:smile)). This kind of model is applied highly succesfully in foreign exchange and credit markets.

```{definition, name="Volatility Smile"}
Volatility smiles are implied volatility patterns that arise in pricing financial options. In particular for a given expiration, options whose strike price differs substantially from the underlying asset's price command higher prices (and thus implied volatilities) than what is suggested by standard option pricing models. These options are said to be either deep in-the-money or out-of-the-money.
```

Furthermore, stochastic volatility models use statistical methods as foundations to price and forecast options' behaviors and the underlying's security volatility is arbitrary. The Heston, the $3/2$ and other models, like the GARCH ^[generalized autoregressive conditional heteroscedasticity. [@engle1982autoregressive, @bollerslev1986generalized]] and SABR ^[stochastic alpha, beta, rho. [@hagan2014arbitrage]], are considered standard smile models.




```{r smile, echo=FALSE, message=FALSE, results='asis', fig.asp = 0.7, fig.width = 5, fig.align='center', fig.cap='Volatility Smile \\label{smile}'}
source("./r_code/smile.R")
```


### Cox-Ingersoll-Ross model {#cir}
 
 The Cox-Ingersoll-Ross (CIR) model is a well-known short-rate model that describes the interest rate movements driven by one source of market risk. The dynamics are described as follows [@cox1985theory; @heston1993closed]:
 
\begin{align}
\label{eq:cir}
dr_t &= k(\theta - r_t)dt + \sigma \sqrt{r_t} dB_t
\end{align} 

\noindent
Where, $r_t$ is the short rate interest described by parameters $\kappa$ the speed of mean reversion, $\theta$  the long-run mean and $\sigma$ the volatility process.

This model has been widely used to describe the dynamics of the short rate interest because it has some fundamental features like intuitive parametrization, nonnegativity and pricing formulas. Besides, it takes account of anticipations, risk aversion, investment alternatives and preferences about consumption timing and allows for detailed predictions about how changes in a wide range of underlying variables affect the term structure[@cox1985theory].
Furthermore, this equation constitutes one of the two Heston model equations with the volatility taking the short rate interest place.


### Heston Model {#hes1}

Heston model was introduced in  1993 by Steven Heston to solve the deterministic volatility problems. It was designed to analize bond and currency options and it introduced the following equations, which represent the dynamics of the stock price and the variance processes under the risk-neutral measure [@gilli_numerical_2011; @heston1993closed]:


\begin{align}
\label{eq:heston}
dS_t &= \mu S_t dt + \sqrt{V_t} S_t dW_t \\
dV_t &=  \kappa (\theta - V_t)dt + \sigma \sqrt{V_t} dB_t
\label{eq:hesvar}
\end{align}



The second equation, as described in Section \@ref(cir), is the CIR model equation. The first equation states the asset price process. $\mu$ is the asset's rate of return,  $dW_{t}$ and $dB_{t}$ are two correlated wiener processes with correlation coefficient of $\rho$. 



<!-- ### Other Models -->

<!-- #### Ornstein-Uhlenbeck -->



<!-- The Ornstein-Uhlenbeck is the earliest recorded SDE. Named after Leonard Ornstein and George Eugene Uhlenbeck, it is a stochastic process that describes the acceleration of a pollen particle in a liquid subject to bombardments by molecules [@ubbo]. As we can observe in equation \@ref(eq:oueq), $x_t$ represents the one dimension velocity of the particle, thus $dx_t$ is the *change* in velocity, in other words, its acceleration. The $- \theta x_t$ component slows down the acceleration and is to be understood as frictional force. Besides, we add a noise $W_t$ with intensity $\sigma$ that models the random bombardment by the molecules.  -->

<!-- \begin{align}  -->
<!-- \label{eq:oueq} -->
<!-- &d x_t = - \theta x_t dt + \sigma d W_t -->
<!-- \end{align} -->

<!-- With $\theta$ and $\sigma$ being positive constants. Expressing in terms of $x_t$ we get: -->


<!-- \begin{align} -->
<!-- x_t = e^{-\theta t} \times \left[ x_0  + \sigma \int_{t=0}^{T} e^{\theta t} d W_s \right] \,. -->
<!-- \end{align} -->

<!-- #### Langevin -->

<!-- The Langevin equation describes a system that consists of the molecular bombardment of a speck of dust on a water surface. We know that the intensity of the bombardement does not depend on the state variables [@kloeden1992; @gard1988].  -->

<!-- \begin{align} -->
<!-- m \frac{dv}{dt} = -\zeta v + \delta F (t)  -->
<!-- \end{align} -->

<!-- $m$ is the mass of the particle, $v$ it's velocity, $-\zeta v$ is the frictional force, which is proportional to the velocity, and $\delta F (t)$ is a *fluctuating* force (random) to the frictional force. -->



## Numerical Methods


Numerical methods are tools that are often applied to solve stochastic differential equations because most of these do not have explicit solution. This means that we are not able to solve these equations using symbolic computation. Although we are unable to find an analitical solution, when facing real problems, the approximation given by a numerical method is often sufficient. Alongside the analytical issue, the need to calculate the SDE's trajectory through time is the main reason why studying numerical methods is so important. An implementation of a numerical method is called a numerical algorithm.

We will simulate sample paths of time discrete approximations implemented in the R programming language [@rlang] that we base on a finite discretization of a time interval $[t_0, T]$. We shall generate approximate values of the sample path for each step contained in the discretized interval [@kloeden1992].

In the fixed step methods, the distance between two contiguous points is $d_i = t_i - t_{i-1} = \frac{T-t_0}{N} \;\;\; i  \leq N \in \mathbb{N}$. $N$ being the time interval partition  number.


According to Kloeden [-@kloeden1992], in the stochastic realm, simulated sample paths can be statistically analysed to find how good an approximation is compared to an exact solution. Moreover, the computational costs such as time and memory increases polynomially with the problem's dimension, which is good, and it is possible to apply variance reduction methods that allow a considerable decrease in the required sample size.




### Convergence 


As soon as we talk about numerical methods we are required to approach the topic of approximations and how to handle them. Methods efficiency receive the name of *convergence order*. In the SDE domain there are two main methods of convergence, that are classified according to their criteria. Firstly, we present the *strong order of convergence*. A method is said to have strong convergence $\delta$  to $Y$ if a time discretized $Y_{\delta}$ of a continous-time process $Y$, with $\delta$ being the maximum time increment of the discretization, and for any fixed time horizon $T$ holds true that [@iacus2009simulation]:

\begin{align*}
\mathbb{E} \mid Y_{\delta}(T) - Y(T) \mid \leq \mathbb{C} \delta^{\gamma}, \,\,\, \forall \delta < \delta_0
\end{align*}

with $\delta_0 > 0$ and $\mathbb{C}$ a constant not depending on $\delta$. Strong convergence addresses the problem of solutions' trajectories. For specific conditions, the Euler method has strong convergence order $\gamma = \frac{1}{2}$.
Furthermore, there is the *weak order of convergence*:



\begin{align*}
\mid  \mathbb{E}p(Y_n) - \mathbb{E}p(Y(\tau)) \mid \leq C \Delta t^{\gamma}
\end{align*}

Strong and weak convergence are not mutually exclusive [@iacus2009simulation]. That means that a method with a given strong order of convergence might have a higher weak order of convergence too. This is the case for the Euler scheme, with a strong order of convergence of $1/2$ and a weak order of $1$ (under some conditions). For a more detailed and rigorous explanation of convergence we recommend consulting [@higham2001].

It is worth noting that, altough schemes have a given convergence order, it is not unusual that they behave better than their order for some SDEs specifications.


### Discretization

We know that convergence is an important feature to a numerical method and studies have found not all time discrete possible approximations of an SDE converge in a useful sense to the solution process as the step size adopted tends toward zero [@clements1973well; @clark1980maximum]. Moreover, particularly for SDEs, some of the more rapidly convergent methods available for ordinary differential equations (ODE) do not work, such as higher order Runge-Kutta methods ^[The euler method is the simplest Runge-Kutta method.]. 

One of the methods that do work for ODEs and SDEs is the Euler method, named after the Swiss mathematician Leonhard Euler. Figure \@ref(fig:euler) shows an example of an implementation for the Newton's cooling law with timestep of 2 seconds compared to its analytical solution. This method (*a.k.a.* forward Euler method) is a first-order numerical procedure. It is the most basic explicit method ^[Explicit methods calculate the state of a system at a later time from the state of the system at the current time. Mathematically we have something like $Y(t+\Delta t)=F(Y(t))\,$.] for numerical integration.



```{r euler, echo=FALSE, message=FALSE, results='asis', fig.asp = 0.7, fig.width = 5, fig.align='center', fig.cap='Analytical x Euler solutions \\label{euler}'}
source("./r_code/euler_comp.R")
```



The method is first-order, as stated above, this means that the error in each step is a proportion of the square of the step size. Also, the global error at a given time is a function of the step size.
We proceed to apply the Euler method to SDEs. Consider the equation:

\begin{align}
dS_t &= \mu(S_t,t) dt + \sigma(S_t,t) dW_t
\end{align}

$dW_t$ is the Brownian motion, $\mu$ and $\sigma$ are functions depending on $S_t$ and $t$, over an interval $[0,T]$, and we want to discretize it as $0 = t_1 < t2 < \cdots < t_m = T$ with increments equally spaced $\Delta t$.

Integrating it from $t$ to $\Delta t$ we have the starting point for our (and any) discretization scheme:

\begin{align}
\label{eq:disc1}
S_{t+\Delta t} &= S_t + \int_{t}^{\Delta t}{\mu(S_u,u)}du + \int_{t}^{\Delta t}{\sigma(S_u,u)} dW_u
\end{align}

To use the Euler discretization is the equivalent of approximating integrals using the left-point rule as in Figure \ref{graph_euler}^[See Kiusalaas [-@kiusalaas2013numerical]], we then have:


\begin{align*}
 \int_{t}^{t+\Delta t}{\mu(S_u,u)} dW_u &\approx \mu(S_t,t) \int_{t}^{t+\Delta t}dW_u\\
&= \mu(S_t,t) (W_{t+\Delta t} - W_t)  \\
 \int_{t}^{t+\Delta t}{\sigma(S_u,u)} dW_u &\approx \sigma(S_t,t) \int_{t}^{t+\Delta t}dW_u 
= \sigma(S_t,t) (W_{t+\Delta t} - W_t)  \\
&= \sigma(S_t,t) \sqrt{\Delta t} Z
\end{align*}

$W_{t+\Delta t}-W_t$ and $\sqrt{\Delta t}Z$ have identical distribution, $Z$ being a standard Gaussian variable. The Euler discretization of equation \@ref(eq:disc1) is then:

\begin{align}
\label{eq:disc2}
S_{t+\Delta t} &= S_t + \mu(S_t,t)\Delta t + \sigma(S_t,t)\sqrt{\Delta t}Z
\end{align}

\begin{figure}
\begin{center}
    \includegraphics[width=100mm]{figure/euler_screen.png}
    \caption{Graphical representation of Euler’s formula}\label{graph_euler}
    \caption*{\scriptsize{Source: Numerical methods in Engineering with Python 3.}}
\end{center}
\end{figure}





#### Euler method - Heston model

We now proceed to apply the method to our model of interest. We retake the equations \@ref(eq:heston) and \@ref(eq:hesvar). We begin showing how to discretize the latter [@higham2001; @iacus2009simulation]:

\begin{align}
\label{eq:hesvareuler}
V_{t+\Delta t} = V_t+ \int_{t}^{t+\Delta t}{\kappa (\theta - V_u) du} + \int_{t}^{t+\Delta t}{\sigma \sqrt{V_u} dB_u}
\end{align}

Which discretized turns out as:

\begin{align*}
 \int_{t}^{t+\Delta t}{\kappa (\theta - V_u)} du &\approx \kappa (\theta - V_t) \Delta t\\
 \int_{t}^{t+\Delta t}{\sigma \sqrt{V_u}} dB_t &\approx \sigma \sqrt{V_t} (W_{t+\Delta t}-W_t)
= \sigma \sqrt{V_t \Delta t} Z_v
\end{align*}



And leaves us with:

\begin{align}
V_{t+\Delta t} = V_t + \kappa (\theta - V_t) \Delta t + \sigma \sqrt{V_t \Delta t} Z_v
\end{align}

$Z_v$ is a standard normal variable. To avoid problems with negative values in $\sqrt{V_t}$ we apply the *full truncation* scheme, which substitutes $V_t$ with $V_t^+ = max(0, V_t)$ ^[Another possible scheme (not used in this work) is the *reflection* scheme where we replace $V_t$ with $\mid V_t \mid$].

For the $S_t$ SDE we proceed similarly:

\begin{align}
\label{eq:heseuler}
S_{t+\Delta t} = S_t+ \mu \int_{t}^{t+\Delta t}{ S_u du} + \int_{t}^{t+\Delta t}{\sqrt{V_u} S_u dW_u}
\end{align}

Discretizing we have:

\begin{align*}
 \int_{t}^{t+\Delta t}{S_u} du &\approx S_t \Delta t\\
 \int_{t}^{t+\Delta t}{\sqrt{V_u} S_u} dW_u &\approx \sqrt{V_t} S_t (W_{t+\Delta t}-W_t)\\
&= \sqrt{V_t \Delta t} S_t Z_s
\end{align*}

$Z_s$ is a standard normal variable with correlation $\rho$ with $Z_v$. We have:

\begin{align}
S_{t+\Delta t} = S_t + \mu S_t \Delta t + \sqrt{V_t \Delta t} S_t Z_s
\end{align}











### Stability

Most differential equations, deterministic or stochastic, cannot be solved
explicitly [@kloeden1992]. Hence, stability studies begin with computers and is associated with numerical methods and approximations. Convergent methods were resulting in bigger errors than what was expected that could not be only due to discretization error. Eventually, scientists discovered that this unexpected problem was caused by accumulation of successive truncation errors. Figure \@ref(fig:stab) retakes the cooling example previously approached to show instability due to an increase in size of the timestep and extending to $600$ seconds. The top plot, with a step of $25$ still converges to the real solution, but we already observe an odd behaviour since the numerical method doesn't follow the analytical solution, but instead revolver around it until convergence. The bottom plot, presents a small increase in the step $h = 29$, and this small increment is enough results in numbers completely off target.

```{r stab, echo=FALSE, message=FALSE, results='asis', fig.height=5, fig.width = 5, fig.align='center', fig.cap='Euler\'s stability whith different timesteps \\label{stab}'}
source("./r_code/stab.R")
```

We know that binary machines like computers are not able to represent all the real numbers, but only a subset of them. Thus, solving these errors is not straightforward since it's not possible to eliminate *all* truncation error when using a computer and dealing with numerical solutions. When faced to an incorrect (not acceptable) solution, we have to evaluate and distinguish between two distinct situations:

\begin{itemize}
  \item [i] Rounding errors are considerably amplified by the algorithm. This situation is called numerical instability.
  \item [ii] Small perturbations of data generate large changes in the solution. This is termed an ill-conditioned (or sensitive) problem.
\end{itemize}

Examples of these two classes of problem can be found in [@gilli_numerical_2011].


Stiff equations appear very often in mathematical problems and refer to differential equations for which a numerical methods might be unstable for not small enough stepsizes [@iserles]. A differential equation of the form $y' = f(t,y)$, if its exact solution $y(t)$ includes a term that decays exponentially to zero as $t$ increases, but whose derivatives are greater in magnitude than the term itself. In other words, if it requires a significant depression of the stepsize to avoid stability lost. This is a loose definition but, since we are dealing with numerical methods a proper mathematical definition isn't required. Tipically, these equations are of the form $e^{-ct}$, where $c$ is a large positive constant [@burden2001]. A practical example of the stiff behavior is the following differential equation [@lambers]:

\begin{align*}
y' = -100y, \,\,\,\,\,\, t > 0, \,\,\,\,\,\, y_0 = 1
\end{align*}

Whose exact solution is $y_t = e^{-100 t}$ and goes to zero as $t$ increases. Applying Euler's method to this equation with $h=0.1$ we stumble in the following equation

\begin{align*}
y_{n+1} = y_{n} - 100 h y_{n} = -9y_{n}
\end{align*}

which is wrong, since it yields an exponencially growing solution $y_n = (-9)^n$. On the other hand, if our timestep is smaller $h = 10^{-3}$, our solution using Euler's method becomes $y_n = (0.9)^n$. This solution leads to an accurate behavior regarding the exact solution, it rapidily decays to zero.



Sometimes, it is interesting to rank differential equations that are more or less stiff. Thus, people compute the quotient of the largest and the smallest eigenvalues of a linear system. They call it the equations' \textit{stiffness ratio} and, usually the bigger the stiffness ratio, the more likely is to be stiff [@iserles].





#### Stability Domain

Let's take the equation:

\begin{align}
y' = \lambda y,  \,\,\,\,\,\, t \geq 0, \,\,\,\,\,\,\,\,\,\,\,\,\,\, y_0 = 1
\end{align}

where $\lambda \in \mathbb{C}$ or in other terms $\lambda = \lambda_r + i \lambda_i$ and whose solution is $y+t = e^{\lambda t}$. We can rewrite this equation as a system:

\begin{align}
\frac{d}{dt} \begin{bmatrix} y^1 \\ y^2 \end{bmatrix} = 
\begin{pmatrix} 
\lambda_r & - \lambda_i \\
\lambda_i & - \lambda_r 
\end{pmatrix}
\begin{bmatrix} y^1 \\ y^2 \end{bmatrix}
\end{align}


The $\lim_{t \to \infty} y_t = 0$ if and only if $\mathbb{R} \left[\lambda \right] < 0$. The *linear stability domain* $\mathcal{D}$ is defined as the set of all numbers $\Delta \lambda \in \mathbb{C}$ such that $\lim_{n \to \infty} y_n = 0$, with $\Delta > 0$ being the stepsize. Or, as stated in Kloeden [-@kloeden1992], the suitable values of the stepsize are expressed in terms of *region of absolute stability*, consisting of the complex numbers $\lambda \Delta$ for which an error in $y_0$ at $t_0$ will not grow in subsequent iterations of the method.

Without entering all the details, for these we recommend [@kloeden1992; @lambers; @iserles], the euler's stability domain  is:

\begin{align*}
\mathcal{D}_{Euler} = z \in \mathbb{C} : \mid 1 + z \mid < 1
\end{align*}

which represents the interior of a complex disc of unit radius and centre $z = -1$ as can be seen in Figure \@ref(stab2), on the left. The right side of the Figure \@ref(stab2) shows the stability region called A-stability ^[Mathematically: $\mathcal{D} \subseteq \left\{ z \in \mathbb{C} : \mathbb{R} z < 0 \right\}$.]. If a method is A-stable, the stepsize $\Delta$ is only constraint by accuracy.


<!-- # ```{r stab_domain, echo = FALSE, eval=TRUE, message=FALSE, fig.height=5, fig.width = 5, fig.align='center'} -->
<!-- # knitr::include_graphics("figure/stab_domain.pdf") -->
<!-- # ``` -->

```{r stab2, echo=FALSE, message=FALSE, results='asis', fig.height=2.7, fig.width = 5.85, fig.align='left', fig.cap='Stability domains \\label{stab2}'}
source("./r_code/stab2.R")
```







Thereby, we claim that stability method study is an important topic, since it enables achieving solutions that are good to stiff equations without having to overly reduce our timesteps which can be very computationally costly.



<!-- ### -->
<!-- [@gilli_numerical_2011] -->
<!--   The precision for finite difference methods depends, among other things, on -->
<!--   the number of time and space steps. These issues are known as the problem -->
<!--   of stability that is caused by an ill-conditioned linear system. We will not -->
<!--   discuss this problem in detail but simply give some intuition. We have to -->
<!--   be aware that increasing the number of space steps increases the size of the -->
<!--   linear system, and the number of time steps defines how often the linear -->
<!--   system has to be solved. Also, the parameters like volatility  and risk-free -->
<!--   rate r influence the conditioning of the linear system. -->

<!--   [@gilli_numerical_2011] -->
<!--   In Section 2.1, it has been shown that binary machines can represent only a -->
<!--   subset of the real numbers, introducing rounding errors that may seriously -->
<!--   affect the precision of the numerical solution. If the “quality” of a solution -->
<!--   is not acceptable, it is important to distinguish between the following two -->
<!--   situations: -->
<!--   i. Rounding errors are considerably amplified by the algorithm. This -->
<!--   situation is called numerical instability. -->
<!--   ii. Small perturbations of data generate large changes in the solution. This -->
<!--   is termed an ill-conditioned (or sensitive) problem. -->
<!--   In the following we give illustrations of numerical instability and of an -->
<!--   ill-conditioned problem. -->
<!-- ### -->

















#######################################






































































<!-- * Model -->


<!-- It's easy to create a list.  It can be unordered like -->

<!-- * Item 1 -->
<!-- * Item 2 -->

<!-- or it can be ordered like -->

<!-- 1. Item 1 -->
<!-- 4. Item 2 -->

<!-- Notice that I intentionally mislabeled Item 2 as number 4.  _Markdown_ automatically figures this out!  You can put any numbers in the list and it will create the list.  Check it out below. -->

<!-- To create a sublist, just indent the values a bit (at least four spaces or a tab).  (Here's one case where indentation is key!) -->

<!-- 1. Item 1 -->
<!-- 1. Item 2 -->
<!-- 1. Item 3 -->
<!--     - Item 3a -->
<!--     - Item 3b -->

<!-- ## Line breaks -->

<!-- Make sure to add white space between lines if you'd like to start a new paragraph.  Look at what happens below in the outputted document if you don't: -->

<!-- Here is the first sentence.  Here is another sentence.  Here is the last sentence to end the paragraph. -->
<!-- This should be a new paragraph. -->

<!-- *Now for the correct way:* -->

<!-- Here is the first sentence.  Here is another sentence.  Here is the last sentence to end the paragraph. -->

<!-- This should be a new paragraph. -->

<!-- ## R chunks -->

<!-- When you click the **Knit** button above a document will be generated that includes both content as well as the output of any embedded **R** code chunks within the document. You can embed an **R** code chunk like this (`cars` is a built-in **R** dataset): -->

<!-- ```{r cars} -->
<!-- summary(cars) -->
<!-- ``` -->

<!-- ## Inline code -->

<!-- If you'd like to put the results of your analysis directly into your discussion, add inline code like this: -->

<!-- > The `cos` of $2 \pi$ is `r cos(2*pi)`. -->

<!-- Another example would be the direct calculation of the standard deviation: -->

<!-- > The standard deviation of `speed` in `cars` is `r sd(cars$speed)`. -->

<!-- One last neat feature is the use of the `ifelse` conditional statement which can be used to output text depending on the result of an **R** calculation: -->

<!-- > `r ifelse(sd(cars$speed) < 6, "The standard deviation is less than 6.", "The standard deviation is equal to or greater than 6.")` -->

<!-- Note the use of `>` here, which signifies a quotation environment that will be indented. -->

<!-- As you see with `$2 \pi$` above, mathematics can be added by surrounding the mathematical text with dollar signs.  More examples of this are in [Mathematics and Science] if you uncomment the code in [Math]. -->

<!-- ## Including plots -->

<!-- You can also embed plots.  For example, here is a way to use the base **R** graphics package to produce a plot using the built-in `pressure` dataset: -->

<!-- ```{r pressure, echo=FALSE, cache=TRUE} -->
<!-- plot(pressure) -->
<!-- ``` -->

<!-- Note that the `echo=FALSE` parameter was added to the code chunk to prevent printing of the **R** code that generated the plot.  There are plenty of other ways to add chunk options.  More information is available at <http://yihui.name/knitr/options/>. -->

<!-- Another useful chunk option is the setting of `cache=TRUE` as you see here.  If document rendering becomes time consuming due to long computations or plots that are expensive to generate you can use knitr caching to improve performance.  Later in this file, you'll see a way to reference plots created in **R** or external figures. -->

<!-- ## Loading and exploring data -->

<!-- Included in this template is a file called `flights.csv`.  This file includes a subset of the larger dataset of information about all flights that departed from Seattle and Portland in 2014.  More information about this dataset and its **R** package is available at <http://github.com/ismayc/pnwflights14>.  This subset includes only Portland flights and only rows that were complete with no missing values.  Merges were also done with the `airports` and `airlines` data sets in the `pnwflights14` package to get more descriptive airport and airline names. -->

<!-- We can load in this data set using the following command: -->

<!-- ```{r load_data} -->
<!-- flights <- read.csv("data/flights.csv") -->
<!-- ``` -->

<!-- The data is now stored in the data frame called `flights` in **R**.  To get a better feel for the variables included in this dataset we can use a variety of functions.  Here we can see the dimensions (rows by columns) and also the names of the columns. -->

<!-- ```{r str} -->
<!-- dim(flights) -->
<!-- names(flights) -->
<!-- ``` -->

<!-- Another good idea is to take a look at the dataset in table form.  With this dataset having more than 50,000 rows, we won't explicitly show the results of the command here.  I recommend you enter the command into the Console **_after_** you have run the **R** chunks above to load the data into **R**. -->

<!-- ```{r view_flights, eval=FALSE} -->
<!-- View(flights) -->
<!-- ``` -->

<!-- While not required, it is highly recommended you use the `dplyr` package to manipulate and summarize your data set as needed.  It uses a syntax that is easy to understand using chaining operations.  Below I've created a few examples of using `dplyr` to get information about the Portland flights in 2014.  You will also see the use of the `ggplot2` package, which produces beautiful, high-quality academic visuals. -->

<!-- We begin by checking to ensure that needed packages are installed and then we load them into our current working environment: -->

<!-- ```{r load_pkgs, message=FALSE} -->
<!-- # List of packages required for this analysis -->
<!-- pkg <- c("dplyr", "ggplot2", "knitr", "bookdown", "devtools") -->
<!-- # Check if packages are not installed and assign the -->
<!-- # names of the packages not installed to the variable new.pkg -->
<!-- new.pkg <- pkg[!(pkg %in% installed.packages())] -->
<!-- # If there are any packages in the list that aren't installed, -->
<!-- # install them -->
<!-- if (length(new.pkg)) -->
<!--   install.packages(new.pkg, repos = "http://cran.rstudio.com") -->
<!-- # Load packages (thesisdown will load all of the packages as well) -->
<!-- library(thesisdown) -->
<!-- ``` -->

<!-- \clearpage -->

<!-- The example we show here does the following: -->

<!-- - Selects only the `carrier_name` and `arr_delay` from the `flights` dataset and then assigns this subset to a new variable called `flights2`. -->

<!-- - Using `flights2`, we determine the largest arrival delay for each of the carriers. -->

<!-- ```{r max_delays} -->
<!-- flights2 <- flights %>% -->
<!--   select(carrier_name, arr_delay) -->
<!-- max_delays <- flights2 %>% -->
<!--   group_by(carrier_name) %>% -->
<!--   summarize(max_arr_delay = max(arr_delay, na.rm = TRUE)) -->
<!-- ``` -->

<!-- A useful function in the `knitr` package for making nice tables in _R Markdown_ is called `kable`.  It is much easier to use than manually entering values into a table by copying and pasting values into Excel or LaTeX.  This again goes to show how nice reproducible documents can be! (Note the use of `results="asis"`, which will produce the table instead of the code to create the table.)  The `caption.short` argument is used to include a shorter title to appear in the List of Tables. -->

<!-- ```{r maxdelays, results="asis"} -->
<!-- kable(max_delays, -->
<!--       col.names = c("Airline", "Max Arrival Delay"), -->
<!--       caption = "Maximum Delays by Airline", -->
<!--       caption.short = "Max Delays by Airline", -->
<!--       longtable = TRUE, -->
<!--       booktabs = TRUE) -->
<!-- ``` -->

<!-- The last two options make the table a little easier-to-read. -->

<!-- We can further look into the properties of the largest value here for American Airlines Inc.  To do so, we can isolate the row corresponding to the arrival delay of 1539 minutes for American in our original `flights` dataset. -->


<!-- ```{r max_props} -->
<!-- flights %>% filter(arr_delay == 1539, -->
<!--                   carrier_name == "American Airlines Inc.") %>% -->
<!--   select(-c(month, day, carrier, dest_name, hour, -->
<!--             minute, carrier_name, arr_delay)) -->
<!-- ``` -->

<!-- We see that the flight occurred on March 3rd and departed a little after 2 PM on its way to Dallas/Fort Worth.  Lastly, we show how we can visualize the arrival delay of all departing flights from Portland on March 3rd against time of departure. -->

<!-- ```{r march3plot, fig.height=3, fig.width=6} -->
<!-- flights %>% filter(month == 3, day == 3) %>% -->
<!--   ggplot(aes(x = dep_time, y = arr_delay)) + geom_point() -->
<!-- ``` -->

<!-- ## Additional resources -->

<!-- - _Markdown_ Cheatsheet - <https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet> -->

<!-- - _R Markdown_ Reference Guide - <https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf> -->

<!-- - Introduction to `dplyr` - <https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html> -->

<!-- - `ggplot2` Documentation - <http://docs.ggplot2.org/current/> -->
