---
output:
  pdf_document: default
  html_document: default
---

<!--
This is for including Chapter 1.  Notice that it's also good practice to name your chunk.  This will help you debug potential issues as you knit.  The chunk above is called intro and the one below is called chapter1.  Feel free to change the name of the Rmd file as you wish, but don't forget to change it here from chap1.Rmd.
-->

<!--
The {#rmd-basics} text after the chapter declaration will allow us to link throughout the document back to the beginning of Chapter 1.  These labels will automatically be generated (if not specified) by changing the spaces to hyphens and capital letters to lowercase.  Look for the reference to this label at the beginning of Chapter 2.
-->


# Literature Review {#lt-review}

This chapter presents the concepts of stochastic calculus, from the historic conception of how it first arose through the basic principles and applications in finance. We address with more care the classical Black-Scholes model and its limitations and the Heston model. This model is also well known, it brings the concept of stochastic volatility in it, which brings its results closer to reality.


[//]: is a generalization of the Riemann-Stieltjes integral 


## Stochastic Calculus

Stochastic calculus arises from stochastic processes and allows the creation of a theory of integration where both the integrand and integrator terms are stochastic processes. Stochastic calculus was created by the Japanese mathematician Kiyosi Itô \footnote{There is another important stochastic integral, called the \textit{Stratonovich Integral} that unlike the Itô's integral, respects the conventional calculus chain rule. Also, the integral is evaluated at the interval's midpoint, instead of its left extreme. A Stratonovich integral can be expressed as an Itô integral and vice versa.} in the 1940s and 1950s and is used for modeling financial options and in another wide variety of fields [@ubbo]. In this chapter we present the historical contexts in which the tools and models used arise, but our focus is introducing the concepts and notations that will be further used in our work.


### The Stochastic differential equation - SDE


At first, before introducing stochastic differential equation, it is helpful to start with ordinary differential equation. Let $x(t) = x_t$ denote the population at time $t$ so that the change in the population at time $t$ is given by the following deterministic differential equation:

\begin{align}
dx_t &= f(t, x_t)dt \\
x(0) &= x_0 \nonumber
\end{align}

We now add a ``noise'' to this equation:

\begin{align} \label{sde1}
dx_t &= \overbrace{\underbrace{f(t, x_t)}_\text{drift}dt}^\text{deterministic} + \overbrace{\underbrace{g(t, x_t)}_\text{diffusion}dW_t}^\text{random shock} \\
x(0) &= x_0 \nonumber
\end{align}

This ``noise''  $dW_t$ is a *random* Wiener process time derivative (which will be clarified below) and $X_0$ is our initial value. 

The $g(t, x_t)$ part of the SDE is often referred as a *diffusion process*. These processes generally have a continuous paths. Before moving on, we must carefully define what the term *random*, means and the best way to begin doing so is to precisely define a probability space:

\begin{definition}{(Probability Space)} A triple ($\Omega$, $\mathcal {U}$, $\mathcal {P}$) is called a \textit{probability space} provided $\Omega$ is any set, $\mathcal {U}$ is a $\sigma$-algebra of subsets of $\Omega$ and $\mathcal {P}$ is a probability measure on $\mathcal {U}$ .
\end{definition}

### Brownian Motion

The Brownian motion is the name given to the irregular motion observed in the motion of pollen particles suspended in fluid resulting from particle collision with atoms or molecules. It is named after Robert Brown, the first to have observed the movement in 1828. He noted two characteristic in the pollen movement [@ubbo]:

* the path of a given particle is very irregular, having a tangent at no point

* the motion of two distinct particles appear to be independent

The first quantitative works in brownian motion come from an interest in stock price fluctuation by Bachelier in 1900. Albert Einstein also leaned over the subject and in 1905 derived the transition density for Brownian motion from molecular-kinetic theory of heat [@ubbo; @karatzas2012brownian].



In 1923, the Wiener process was coined in honor of Norbert Wiener mathematical proof of existence of the brownian motion and stating its properties ^[More can be found on [@helgadottir2016option; @evans; @rosenthal].].


\begin{definition}{(Wiener Process)} Given a probability space ($\Omega$, $\mathcal {U}$, $\mathcal {P}$), a stochastic process $W_t$ defined in this space is a \textit{wiener process} if it satisfies the following properties:

\begin{itemize}
  \item  $W_{0}=0$
  
  \item The change in $W$, given by $\Delta W = W_{t+1}-W_{t}$, is normally distributed with mean zero and standard deviation $\sqrt{\Delta t}$, meaning that $\Delta W = \epsilon\sqrt{\Delta t}$, where $\epsilon$ is $N(0,1)$.
  
  \item If the increment $\Delta t_1$ does not overlap with the time increment $\Delta t_2$, then $\Delta W_1$ and $\Delta W_2$ are independent.
  
  \item The process is continuous, meaning that there are no jumps in the process.
  
  \item The process is a Markov process. This means that the conditional expectation of $W_{t+1}$ given its entire history is equal to the conditional expectation of $W_{t+1}$ given today's information. This can be written as: $E[W_{t+1}|W_1, ..., W_t] = E[W_{t+1}|W_t]$.
  
  \item Consider the time interval $[0,t]$ with $n$ equally spaced intervals given by $t_i = \frac{it}{n}$. Then the paths of the Brownian motion have unbounded variation, this means that they are not differentiable and go towards infinity as $n$ increases. The quadratic variation is given by $\sum_{i=1}^{n}{(Z_{t_i}-Z_{t_{i-1}})^2} \rightarrow t$, meaning that when $n$ increases it stays constant at $t$. 

\end{itemize}
\end{definition}

<!-- \ref{wiener} -->
<!-- \@ref(fig:wiener) -->

```{r wiener, echo=FALSE, message=FALSE, results='asis', fig.asp = 0.7, fig.width = 5, fig.align='center', fig.cap='A Wiener process trajectory path example \\label{wiener}'}
source("./r_code/wiener.R")
```



#### Correlated Brownian Motions {#corr}

Two independent brownian motions that are correlated can describe a new process $Z_t$. Let $W_1$ and $W_2$ be these two *independent* Brownian motions and let $-1 \leq \rho \leq 1$ be a given number. For $0 \leq t \leq T$ define the new process $Z_t$ as [@ubbo]:

\begin{align}
\label{eq:corr_brow}
Z_t = \rho W_{1,t} + \sqrt{1-\rho^2}W_{2,t}
\end{align}

\noindent
This equation is a linear combination of independent normals at each timestep $t$, so $Z_t$ is normally distributed. It is proven that $Z$ is a Brownian motion and that $Z$ and $W_{1,t}$ are correlated [@ubbo, pp. 514].





### Itô's Integral


Formally, the SDE  presented in equation \ref{sde1} only exists because we can rewrite it in the form [@tong2012option; @evans; @steele2012stochastic; @ito1951; @ito1962; @rosenthal]:

\begin{align} \label{sde1}
x_t &= x_0 + \int_{0}^{t}{f(s, x_s)ds} + \int_{0}^{t}{g(s, x_s)dW_s}
\end{align}


\noindent
for some $f(s, x_s)$, $g(s, x_s)$ and $s \in [0,t]$.



#####################


The Itô integral can, as the Riemann integral, be approximated by a finite sum. Also, it has a definition as a certain limit. Itô's lemma \@ref(itolemma) plays the same role as the fundamental theorem of calculus in allowing to evaluate integrals. It is the formal definition and presents an extra term not encountered in the conventional calculus theorem that is due to the non-smoothness characteristics of Brownian motion paths. It is possible, though, to define the integral in a less rigorous way: 

\begin{align}
Y_{\Delta t}(t) \approx \sum_{t_k < t}{g(t_k)\Delta W_k}
\end{align}

\noindent
with the usual notions $t_k = k\Delta t$, and $\Delta W_k = W(t_{k+1})-W(t_k))$. And in a more rigorous form, if the limit exists, then the Ito integral is:

\begin{align}
Y(t)  = \lim\limits_{\Delta t \to 0} Y_{\Delta t}(t)
\end{align}


It is essential that the *forward difference* is used rather than the backward difference, which would be **wrong**.


\begin{theorem}[Itô's Lemma] \label{itolemma}
Assume that $S_t$ has a stochastic differential given by:

\begin{align}
dS_t = \mu_t dt + \sigma_t dW_t 
\end{align}

\noindent
for $\mu_t$, $\sigma_t$ and $t \in [0,T]$. Assume $u: \mathbb{R} \times [0, T] \rightarrow \mathbb{R}$ is continuous and that $\frac{\partial u}{\partial t}$, $\frac{\partial u}{\partial x}$, $\frac{\partial^2 u}{\partial x^2}$ exist and are continuous.

\begin{align*}
Y_t := u(S_t, t)
\end{align*}


\noindent
Then Y has the following stochastic differential:

\begin{align} 
\label{eq:ito}
\begin{split}
    dY_t &= \frac{\partial u}{\partial t}dt + \frac{\partial u}{\partial x} dS_t + \frac{1}{2}\frac{\partial^2 u}{\partial x^2}\sigma_t^2 dt  \\[10pt] 
    &= \left( \frac{\partial u}{\partial t} + \mu_t \frac{\partial u}{\partial x} + \frac{1}{2}\frac{\partial^2 u}{\partial x^2}\sigma_t^2 \right) dt + \sigma_t \frac{\partial u}{\partial x} dW_t
\end{split}
\end{align}

\noindent 
where the argument of $u$, $\frac{\partial u}{\partial x}$ and $\frac{\partial^2 u}{\partial x^2}$ above is $\left( S_t, t \right)$ .
\end{theorem}

Equation \@ref(eq:ito) is the stochastic equivalent to the chain rule, also known as Itô's formula or Itô's chain rule. The proof to this theorem is based on the Taylor expansion of the function $f(S_t, t)$ [@tong2012option; @evans]. For practical uses you should write out a second-order Taylor expansion for the function to be analyzed and apply the     \@ref(tab:box-calc)  multiplication table [@ubbo].

```{r box-calc, echo=FALSE, message=FALSE, results='asis'}
source("./r_code/box.r")
```



#### Itô's Integral Properties

Let $f$, $g$ $\in$ $\mathcal{V}$ and let $0 \leq t_0 < u < T$. Then

\begin{enumerate}[label=(\roman*)]
  \item $\int_{t_0}^{T}{f dB_t} = \int_{t_0}^{u}{f dB_t} + \int_{u}^{T}{f dB_t}$
  \item $\int_{t_0}^{T}{(\alpha f + \beta g) dB_t} = \alpha  \int_{t_0}^{T}{f dB_t} + \int_{t_0}^{T}{ \beta g dB_t}$
  \item $\mathbb{E}\left[ \int_{t_0}^{T}{fdB_t}\right] = 0 $
  \item $\mathbb {E} \left[\left(\int_{0}^{t}H_{s}\,dB_{s}\right)^{2}\right]=\mathbb {E} \left[\int _{0}^{t}H_{s}^{2}\,ds\right]$ (Isometry)
  \item $ \mathbb {E}\left[ \int_{t_0}^{T}{f dB_t \mid \mathcal{F}_{s}} \right] = \int_{t_0}^{s}{f dB_t}, \,\,\,\,\,\,\,\,\,\,\,\,\,\, for \,\, s < T.$  (Martingale\footnote{A martingale is a stochastic process with certain characteristics. The main one is that the expected value in time $t+1$ for $X$ is the $X$ value in $t$. This means there are no winning strategies when we are dealing with martingales (unlike when we play poker, for example). A Wiener process is a martingale.})
\end{enumerate}


 





## Black-Scholes Model

### Basics


The Black-Scholes (B-S) model arises from the need to price european options in the derivative markets. Derivatives are financial instruments traded in the market, stock exchange or over-the-counter (OTC) market, whose values depend on the values of an underlying asset. [@black1973pricing; @yang2013valuing; @salomao2011precificaccao]

* A call option is a derivative that gives its bearer the right, but not the obligation, to purchase a specific asset by a fixed price before or on a given date. 

* A put option is a derivative that gives its bearer the right, but not the obligation, to sell a specific asset by a fixed price before or on a given date.

The trading price of the option is called the option *premium* and the asset from which the option derives is called the *underlying asset*. This asset may be the interest rate, exchange rates, stock exchanges rates, commodities or stocks. The fixed price in contract in which the underlying asset might to be bought or sold is the *strick price*. The option expiration date is called the *maturity*. [@salomao2011precificaccao; @black1973pricing]

There are two major different option types: the European and the American. The difference between these two is that the bearer of the first may exercise it only at the end of its life, at its maturity while the latter can be exercised at any given time until its maturity. [@black1973pricing; @merton1973theory]


\begin{definition}{(Implicit volatility)} 
Given all the option's parameters in a precification model and its market price, the option's volatility is called the \textit{implicit volatility}.
\end{definition}



\begin{definition}{(Intrinsic value)} 
The intrinsic value of a call is the difference between the underlying asset price and the strike price. The put's intrinsic value operates the other way around, being the difference between the strike and the underling asset prices.
\end{definition}




















#### Geometric Brownian Motion {#gbm}

A stochastic process $S_t$ is a geometric brownian motion ^[There is an arithmetic brownian motion, whose equation is: $dS_t = \mu dt + \sigma dB_t$. More information can be obatined about this process looking at [@ubbo].] if its solution is described by the solution of the following stochastic differential equation [@ubbo; @tong2012option; @tsay2005analysis].

\begin{align}
dS_t = \mu S_t dt + \sigma S_t dW_t
\end{align}

\noindent
for given constants $\mu \in {\rm I\!R}$ and $\sigma > 0$. Also, the assumed initial value is positive, $S_0 >0$.

This process ^[Also known as exponential brownian motion.] (Figure \ref{gbm}) is used quite often in finance to model the dynamics of some assets because of its properties. It has independent multiplicative increments and is the process used to price options in the Black-Scholes model [@iacus2009simulation]:


\begin{align}
S_t = S_0 \times exp{\left(\mu - \frac{\sigma^2}{2} \right) t + \sigma W_t}, \;\; t > 0
\end{align}


```{r gbm, echo=FALSE, message=FALSE, results='asis', fig.asp = 0.7, fig.width = 5, fig.align='center', fig.cap='A GBM trajectory path example \\label{gbm}'}
source("./r_code/gbm.R")
```









### The model

The Black-Scholes model provides analytical solution to the price of a European call at time $t$  and can be described as follows [@yang2013valuing; @black1973pricing; @helgadottir2016option]:

\begin{align}
C(S_{t},t)&=N(d_{1})S_{t}-N(d_{2})Ke^{-r(T-t)}\\[10pt]
d_{1}&={\frac {1}{\sigma {\sqrt {T-t}}}}\left[\ln \left({\frac {S_{t}}{K}}\right)+\left(r+{\frac {\sigma ^{2}}{2}}\right)(T-t)\right]\\[10pt]
d_{2}&=d_{1}-\sigma {\sqrt {T-t}}
\end{align}




\noindent
Where:

* $S_{t}$ is the spot price of the underlying asset at time $t$
* $r$ is the risk free rate (generally an annual rate)\footnote{Assumed to be constant. \label{teste}}
* $\sigma$ is the volatility of returns of the underlying asset \footnote{See footnote 1.}
* $N(\cdot )$ is the cumulative distribution function of the standard Gaussian distribution
* $K$ is the strike price
* $T-t$ is the time to maturity 


\noindent
Also, the stock price path is a Geometric Brownian Motion as previously stated, and is under the risk-neutral measure with the following dynamics [@helgadottir2016option; @nmof]: 

\begin{align}
dS_{t} = (r-q)S_td_t+\sigma S_t dW_t
\end{align}

\noindent
Where $dW_t$ is a Wiener process [@black1973pricing; @nmof], $r$ is the risk free rate and $q$ is the dividend yield\footnote{$r$ and $q$ are assumed to be constant.} and $t$ denotes the current point in time. 





### Limitations


Although the Black-Scholes is very popular and the *de facto* standard in the market there are implications to the B-S model assumptions that affect the results and that are unrealistic. The main assumption that does not hold up is the deterministic (constant) volatility, that can more accurately be described as a stochastic process since we observe that small moves usually are followed by small moves and large moves by large moves.   [@yang2013valuing; @helgadottir2016option]

Other assumptions that are critical to the B-S model and are not always observed in practice refer to the asset's continuity through time (no jumps), being allowed to perform continuous hedge without transactions costs and normal (Gaussian) returns.

Most models focus on the volatility problem because transaction costs often translate to rises in volatility and fat-tails (abnormal) returns can be simulated by stochastic volatility and market or volatility jumps.

<!-- Out of all the parameters that the call price depends on in the Black Scholes -->
<!-- model, the volatility is the only one that is unobservable, but we can let the -->
<!-- market of liquid derivatives decide which volatility has to be plugged in the -->
<!-- model. The calibrated σ* is the one that fits best the model prices to the observed -->
<!-- market prices, by minimizing the sum of square errors: -->
<!-- σ ∗ = arg q 	min -->
<!-- 2,o -->
<!-- C mn S " , K 2 , σ, T o − C p S", K 2 , T o -->
<!-- . -->
<!-- Knowing S, K, T, r, q and the quoted option call value it is possible to extract the -->
<!-- implied volatility. -->




<!-- [@tong2012option] -->
<!-- Using Black-Scholes option pricing model, the price of a call option is the function of -->
<!-- the spot (current) price S(0), interest rate r, the strike K, the constant volatility σ -->
<!-- and the maturity T . Except for the volatility σ, all the other variables are observable. -->
<!-- Since the quoted option price C obs is observable, using the Black-Scholes formula we -->
<!-- can therefore calculate or imply the volatility that is consistent with the quoted his- -->
<!-- torical option prices and observed variables. We can therefore define implied volatility -->
<!-- σimpl by -->
<!-- C BS (0; r, K, T, σimpl , S(0)) = C obs -->
<!-- where C BS is the option price calculated by the Black-Scholes formula (equation -->
<!-- (3.3.9)). Implied volatility surfaces are graphs plotting σimpl for each call option’s -->
<!-- strike K and expiration T . Theoretically, options whose underlying is governed by -->
<!-- the geometric Brownian motion should have a flat implied volatility surface, since -->
<!-- volatility is a constant. -->


<!-- [@yang2013valuing] -->
<!-- Volatility is a measure for variation of price of a stock over time. Stochastic volatility is described as -->
<!-- processes in which the return variation dynamics include an unobservable shock that cannot be predicted -->
<!-- using current available information. Stochastic volatility models, which let the volatility follow Brownian -->
<!-- motion, make the option price much better adapted to the realities of the market. -->


## Stochastic Volatility models

Introducing stochastic volatility to models brings complexity, but enables modeling some features observed in reality that are crucial, like the randomic market volatility effects, skewness (market returns are more realistically modeled) and volatility smile ^[The name derives from the concave shape of the graph, which resembles a smile.] (see Figure \@ref(fig:smile)). This kind of model is applied highly succesfully in foreign exchange and credit markets.

\begin{definition}{(Volatility Smile)} 
Volatility smiles are implied volatility patterns that arise in pricing financial options. In particular for a given expiration, options whose strike price differs substantially from the underlying asset's price command higher prices (and thus implied volatilities) than what is suggested by standard option pricing models. These options are said to be either deep in-the-money or out-of-the-money.
\end{definition}

Furthermore, stochastic volatility models use statistical methods as foundations to price and forecast options' behaviors and the underlying's security volatility is arbitrary. The Heston, the $3/2$ and other models, like the GARCH ^[generalized autoregressive conditional heteroscedasticity.] and SABR ^[stochastic alpha, beta, rho.], are considered standard smile models.




```{r smile, echo=FALSE, message=FALSE, results='asis', fig.asp = 0.7, fig.width = 5, fig.align='center', fig.cap='Volatility Smile \\label{smile}'}
source("./r_code/smile.R")
```


### Cox-Ingersoll-Ross model {#cir}
 
 The Cox-Ingersoll-Ross (CIR) model is a well-known short-rate model that describes the interest rate movements driven by one source of market risk. The dynamics are described as follows[@cox1985theory; @heston1993closed]:
 
\begin{align}
\label{eq:cir}
dr_t &= k(\theta - r_t)dt + \sigma \sqrt{r_t} dB_t
\end{align} 

\noindent
Where, $r_t$ is the short rate interest described by parameters $\kappa$ the speed of mean reversion, $\theta$  the long-run mean variance and $\sigma$ the volatility of the variance process.

This model has been widely used to describe the dynamics of the short rate interest because it has some fundamental features like intuitive parametrization, nonnegativity and pricing formulas. Besides, it takes account of anticipations, risk aversion, investment alternatives and preferences about consumption timing and allows for detailed predictions about how changes in a wide range of underlying variables affect the term structure[@cox1985theory].
Furthermore, this equation constitutes one of the two Heston model equations with the volatility taking the short rate interest place.


### Heston Model {#hes1}

Heston model was introduced in  1993 by Steven Heston to solve the deterministic volatility problems. It was designed to analize bond and currency options and it introduced the following equations, which represent the dynamics of the stock price and the variance processes under the risk-neutral measure [@gilli_numerical_2011; @heston1993closed]:


\begin{align}
\label{eq:heston}
dS_t &= \mu S_t dt + \sqrt{V_t} S_t dW^*_t \\
dV_t &=  \kappa (\theta - V_t)dt + \sigma \sqrt{V_t} dB_t
\label{eq:hesvar}
\end{align}



The second equation, as described in Section \@ref(cir), is the CIR model equation. The first equation states the asset price process. $\mu$ is the asset's rate of return,  $dW_{t,1}$ and $dW_{t,2}$ are two correlated wiener processes with correlation coefficient of $\rho$. 



### Other Models

#### Ornstein-Uhlenbeck



The Ornstein-Uhlenbeck is the earliest recorded SDE. Named after Leonard Ornstein and George Eugene Uhlenbeck, it is a stochastic process that describes the acceleration of a pollen particle in a liquid subject to bombardments by molecules [@ubbo]. As we can observe in equation \@ref(eq:oueq), $x_t$ represents the one dimension velocity of the particle, thus $dx_t$ is the *change* in velocity, in other words, its acceleration. The $- \theta x_t$ component slows down the acceleration and is to be understood as frictional force. Besides, we add a noise $W_t$ with intensity $\sigma$ that models the random bombardment by the molecules. 

\begin{align} 
\label{eq:oueq}
&d x_t = - \theta x_t dt + \sigma d W_t
\end{align}

With $\theta$ and $\sigma$ being positive constants. Expressing in terms of $x_t$ we get:


\begin{align}
x_t = e^{-\theta t} \times \left[ x_0  + \sigma \int_{t=0}^{T} e^{\theta t} d W_s \right] \,.
\end{align}

#### Langevin

The Langevin equation describes a system that consists of the molecular bombardment of a speck of dust on a water surface. We know that the intensity of the bombardement does not depend on the state variables [@kloeden1992]. 

\begin{align}
m \frac{dv}{dt} = -\zeta v + \delta F (t) 
\end{align}

$m$ is the mass of the particle, $v$ it's velocity, $-\zeta v$ is the frictional force, which is proportional to the velocity, and $\delta F (t)$ is a *fluctuating* force (random) to the frictional force.



## Numerical Methods


Numerical methods are tools that are often applied to solve stochastic differential equations because most of these do not have explicit solution. This means that we are not able to solve these equations using symbolic computation. Although we are unable to find an analitical solution, when facing real problems, the approximation given by a numerical method is often sufficient. Alongside the analytical issue, the need to calculate the SDE's trajectory through time is the main reason why studying numerical methods is so important. An implementation of a numerical method is called a numerical algorithm.

We will simulate sample paths of time discrete approximations implemented in the R programming language [@rlang] that we base on a finite discretization of a time interval $[t_0, T]$. We shall generate approximate values of the sample path for each step contained in the discretized interval [@kloeden1992].

In the fixed step methods, the distance between two contiguous points is the distance $d_i = t_i - t_{i-1} = \frac{T-t_0}{N} \;\;\; \forall i \mid 1 \leq i  \leq N \in \mathbb{N}$. $N$ being the time interval partition  number.


According to Kloeden [-@kloeden1992], in the stochastic realm, simulated sample paths can be statistically analysed to find how good an approximation is compared to an exact solution. Moreover, the computational costs suach as time and memory increases polynomially with the problem's dimension, which is good, and it is possible to apply variance reduction methods that allow a considerable decrease in the required sample size.




### Convergence 


As soon as we talk about numerical methods we are required to approach the topic of approximations and how to handle them. Methods efficiency receive the name of *convergence order*. In the SDE domain there are two main methods of convergency, that are classified according to their criteria. Firstly, we present the *strong order of convergence*. A method is said to have strong convergence $\delta$  to $Y$ if a time discretized $Y_{\delta}$ of a continous-time process $Y$, with $\delta$ being the maximum time increment of the discretization, and for any fixed time horizon $T$ holds true that [@iacus2009simulation]:

\begin{align*}
\mathbb{E} \mid Y_{\delta}(T) - Y(T) \mid \leq C \delta^{\gamma}, \,\,\, \forall \delta < \delta_0
\end{align*}

with $\delta_0 > 0$ and $\mathcal{C}$ a constant not depending on $\delta$. Strong convergence addresses the problem of solutions' trajectories. For specific conditions, the Euler method has strong convergence order $\gamma = \frac{1}{2}$.
Furthermore, there is the *weak order of convergence*. The weak convergence 



\begin{align*}
\mid  \mathbb{E}p(Y_n) - \mathbb{E}p(Y(\tau)) \mid \leq C \Delta t^{\gamma}
\end{align*}



For a more detailed and rigorous explanation of convergence we recommend consulting Higham [-@higham2001].


Em específico, quando tratamos de modelos estocásticos, os dois principais tipos
de convergência são a convergência forte, que tange aproximações das trajetórias de
soluções de EDEs em um intervalo de tempo, e a convergência fraca, que diz respeito à
aproximações de EDEs à distribuições correspondentes. Para definir convergência forte
e fraca, vamos primeiro definir os padrões de erro utilizados.


<!-- [@iacus2009simulation] -->
<!-- Simulation methods are usually based on discrete approximations of the -->
<!-- continuous solution to a stochastic differential equation. The methods of approximation are classified according to their different properties. Mainly two -->
<!-- criteria of optimality are used in the literature: the strong and the weak (orders -->
<!-- of ) convergence. -->


<!-- [@higham2001] -->
<!-- In the example above -->
<!-- withem.mthe EM solution matches the true solution more closely as ∆tis decreased— -->
<!-- convergence seems to take place. Keeping in mind thatX(τn) and Xnare random -->
<!-- variables, in order to make the notion of convergence precise we must decide how to -->
<!-- measure their difference. UsingE|Xn−X(τn)| -->


### Discretization

We know that convergence is an important feature to a numerical method and studies have found not all time discrete possible approximations of an SDE converge in a useful sense to the solution process as the step size adopted tends toward zero [@clements1973well; @clark1980maximum]. Moreover, particularly for SDEs, some of the more rapidly convergent methods available for ordinary differential equations (ODE) do not work, such as higher order Runge-Kutta methods ^[The euler method is the simplest Runge-Kutta method.]. 

One of the methods that do work for ODEs and SDEs is the Euler method, named after the Swiss mathematician Leonhard Euler. Figure \@ref(fig:euler) shows an example of an implementation for the Newton's cooling law with timestep of 2 seconds compared to its analytical solution. This method (*a.k.a.* forward Euler method) is a first-order numerical procedure. It is the most basic explicit method ^[Explicit methods calculate the state of a system at a later time from the state of the system at the current time. Mathematically we have something like $Y(t+\Delta t)=F(Y(t))\,$.] for numerical integration.



```{r euler, echo=FALSE, message=FALSE, results='asis', fig.asp = 0.7, fig.width = 5, fig.align='center', fig.cap='Analytical x Euler solutions \\label{euler}'}
source("./r_code/euler_comp.R")
```



The method is first-order, as stated above, this means that the error in each step is a proportion of the square of the step size. Also, the global error at a given time is a function of the step size.
We proceed to apply the Euler methos to SDEs. Consider the equation:

\begin{align}
dS_t &= \mu(S_t,t) dt + \sigma(S_t,t) dW_t
\end{align}

$dW_t$ is the Brownian motion, $\mu$ and $\sigma$ are functions depending on $S_t$ and $t$, over an interval $[0,T]$, and we want to discretize it as $0 = t_1 < t2 < \cdots < t_m = T$ with increments equally spaced $d_t$.

Integrating it from $t$ to $dt$ we have the starting point for our (and any) discretization scheme:

\begin{align}
\label{eq:disc1}
S_{t+dt} &= S_t + \int_{t}^{dt}{\mu(S_u,u)}du + \int_{t}^{dt}{\sigma(S_u,u)} dW_u
\end{align}

To use the Euler discretization is the equivalent of approximating integrals using the left-point rule, we then have:


\begin{align*}
 \int_{t}^{t+dt}{\mu(S_u,u)} dW_u &\approx \mu(S_t,t) \int_{t}^{t+dt}dW_u\\
&= \mu(S_t,t) (W_{t+dt} - W_t)  \\
 \int_{t}^{t+dt}{\sigma(S_u,u)} dW_u &\approx \sigma(S_t,t) \int_{t}^{t+dt}dW_u\\
&= \sigma(S_t,t) (W_{t+dt} - W_t)  \\
&= \sigma(S_t,t) \sqrt{dt} Z
\end{align*}

$W_{t+dt}-W_t$ and $\sqrt{dt}Z$ have identical distribution, $Z$ being a standard gaussian variable. The Euler discretization of equation \@ref(eq:disc1) is then:

\begin{align}
\label{eq:disc2}
S_{t+dt} &= S_t + \mu(S_t,t)dt + \sigma(S_t,t)\sqrt{dt}Z
\end{align}


#### Euler method - Heston model

We now proceed to apply the method to our model of interest. We retake the equations \@ref(eq:heston) and \@ref(eq:hesvar). We begin showing how to discretize the latter [@higham2001; @iacus2009simulation]:

\begin{align}
\label{eq:hesvareuler}
V_{t+dt} = V_t+ \int_{t}^{t+dt}{\kappa (\theta - V_u) du} + \int_{t}^{t+dt}{\sigma \sqrt{V_u} dB_u}
\end{align}

Which discretized turns out as:

\begin{align*}
 \int_{t}^{t+dt}{\kappa (\theta - V_u)} du &\approx \kappa (\theta - V_t) dt\\
 \int_{t}^{t+dt}{\sigma \sqrt{V_u}} dB_t &\approx \sigma \sqrt{V_t} (W_{t+dt}-W_t)\\
&= \sigma \sqrt{V_t dt} Z_v
\end{align*}



And leaves us with:

\begin{align}
V_{t+dt} = V_t + \kappa (\theta - V_t) dt + \sigma \sqrt{V_t dt} Z_v
\end{align}

$Z_v$ is a standard normal variable. To avoid problems with negative values in $\sqrt{V_t}$ we apply the *full truncation* scheme, which substitutes $V_t$ with $V_t^+ = max(0, V_t)$ ^[Another possible scheme (not used in this work) is the *reflection* scheme where we replace $V_t$ with $\mid V_t \mid$].

For the $S_t$ SDE we proceed similarly:

\begin{align}
\label{eq:heseuler}
S_{t+dt} = S_t+ \mu \int_{t}^{t+dt}{ S_u du} + \int_{t}^{t+dt}{\sqrt{V_u} S_u dW_u}
\end{align}

Discretizing we have:

\begin{align*}
 \int_{t}^{t+dt}{S_u} du &\approx S_t dt\\
 \int_{t}^{t+dt}{\sqrt{V_u} S_u} dW_u &\approx \sqrt{V_t} S_t (W_{t+dt}-W_t)\\
&= \sqrt{V_t dt} S_t Z_s
\end{align*}

$Z_s$ is a standard normal variable with correlation $\rho$ with $Z_v$. We have:

\begin{align}
S_{t+dt} = S_t + \mu S_t dt + \sqrt{V_t dt} S_t Z_s
\end{align}











### Stability

Stability studies begin with computers and is associated with numerical methods and approximations. Convergent methods were resulting in bigger errors than what was expected that could not be only due to discretization error. Eventually, scientists discovered that this unexpected problem was caused by accumulation of successive truncation errors.

```{r stab, echo=FALSE, message=FALSE, results='asis', fig.height=5, fig.width = 5, fig.align='center', fig.cap='Euler\'s stability whith different timesteps \\label{stab}'}
source("./r_code/stab.R")
```

We know that binary machines like computers are not able to represent all the real numbers, but only a subset of them. Thus, solving these errors is not straightforward since it's not possible to eliminate *all* truncation error when using a computer and dealing with numerical solutions. When faced to an incorrect (not acceptable) solution, we have to evaluate and distinguish between two distinct situations [@gilli_numerical_2011]:

\begin{itemize}
  \item [i] Rounding errors are considerably amplified by the algorithm. This situation is called numerical instability.
  \item [ii] Small perturbations of data generate large changes in the solution. This is termed an ill-conditioned (or sensitive) problem.
\end{itemize}





[@kloeden1992] Stochastic Stability
Most differential equations, deterministic or stochastic, cannot be solved
explicitly. Nevertheless we can often deduce alot of useful information, usually
qualitative, about the behaviour of their solutions from the functional form of
their coefficients. 

Of particular interest in applications is the long term
asymptotic behaviour and sensitivity of the solutions to small changes, for example measurement errors, in the initial values. From existence and uniqueness
theory we know that the solutions of a differential equation are continuous in their initial values, at least over a finite time interval. Extending this idea to an infinite time interval leads to the concept of stability.




<!-- ### -->
<!-- [@gilli_numerical_2011] -->
<!--   The precision for finite difference methods depends, among other things, on -->
<!--   the number of time and space steps. These issues are known as the problem -->
<!--   of stability that is caused by an ill-conditioned linear system. We will not -->
<!--   discuss this problem in detail but simply give some intuition. We have to -->
<!--   be aware that increasing the number of space steps increases the size of the -->
<!--   linear system, and the number of time steps defines how often the linear -->
<!--   system has to be solved. Also, the parameters like volatility  and risk-free -->
<!--   rate r influence the conditioning of the linear system. -->

<!--   [@gilli_numerical_2011] -->
<!--   In Section 2.1, it has been shown that binary machines can represent only a -->
<!--   subset of the real numbers, introducing rounding errors that may seriously -->
<!--   affect the precision of the numerical solution. If the “quality” of a solution -->
<!--   is not acceptable, it is important to distinguish between the following two -->
<!--   situations: -->
<!--   i. Rounding errors are considerably amplified by the algorithm. This -->
<!--   situation is called numerical instability. -->
<!--   ii. Small perturbations of data generate large changes in the solution. This -->
<!--   is termed an ill-conditioned (or sensitive) problem. -->
<!--   In the following we give illustrations of numerical instability and of an -->
<!--   ill-conditioned problem. -->
<!-- ### -->

















#######################################






































































<!-- * Model -->


<!-- It's easy to create a list.  It can be unordered like -->

<!-- * Item 1 -->
<!-- * Item 2 -->

<!-- or it can be ordered like -->

<!-- 1. Item 1 -->
<!-- 4. Item 2 -->

<!-- Notice that I intentionally mislabeled Item 2 as number 4.  _Markdown_ automatically figures this out!  You can put any numbers in the list and it will create the list.  Check it out below. -->

<!-- To create a sublist, just indent the values a bit (at least four spaces or a tab).  (Here's one case where indentation is key!) -->

<!-- 1. Item 1 -->
<!-- 1. Item 2 -->
<!-- 1. Item 3 -->
<!--     - Item 3a -->
<!--     - Item 3b -->

<!-- ## Line breaks -->

<!-- Make sure to add white space between lines if you'd like to start a new paragraph.  Look at what happens below in the outputted document if you don't: -->

<!-- Here is the first sentence.  Here is another sentence.  Here is the last sentence to end the paragraph. -->
<!-- This should be a new paragraph. -->

<!-- *Now for the correct way:* -->

<!-- Here is the first sentence.  Here is another sentence.  Here is the last sentence to end the paragraph. -->

<!-- This should be a new paragraph. -->

<!-- ## R chunks -->

<!-- When you click the **Knit** button above a document will be generated that includes both content as well as the output of any embedded **R** code chunks within the document. You can embed an **R** code chunk like this (`cars` is a built-in **R** dataset): -->

<!-- ```{r cars} -->
<!-- summary(cars) -->
<!-- ``` -->

<!-- ## Inline code -->

<!-- If you'd like to put the results of your analysis directly into your discussion, add inline code like this: -->

<!-- > The `cos` of $2 \pi$ is `r cos(2*pi)`. -->

<!-- Another example would be the direct calculation of the standard deviation: -->

<!-- > The standard deviation of `speed` in `cars` is `r sd(cars$speed)`. -->

<!-- One last neat feature is the use of the `ifelse` conditional statement which can be used to output text depending on the result of an **R** calculation: -->

<!-- > `r ifelse(sd(cars$speed) < 6, "The standard deviation is less than 6.", "The standard deviation is equal to or greater than 6.")` -->

<!-- Note the use of `>` here, which signifies a quotation environment that will be indented. -->

<!-- As you see with `$2 \pi$` above, mathematics can be added by surrounding the mathematical text with dollar signs.  More examples of this are in [Mathematics and Science] if you uncomment the code in [Math]. -->

<!-- ## Including plots -->

<!-- You can also embed plots.  For example, here is a way to use the base **R** graphics package to produce a plot using the built-in `pressure` dataset: -->

<!-- ```{r pressure, echo=FALSE, cache=TRUE} -->
<!-- plot(pressure) -->
<!-- ``` -->

<!-- Note that the `echo=FALSE` parameter was added to the code chunk to prevent printing of the **R** code that generated the plot.  There are plenty of other ways to add chunk options.  More information is available at <http://yihui.name/knitr/options/>. -->

<!-- Another useful chunk option is the setting of `cache=TRUE` as you see here.  If document rendering becomes time consuming due to long computations or plots that are expensive to generate you can use knitr caching to improve performance.  Later in this file, you'll see a way to reference plots created in **R** or external figures. -->

<!-- ## Loading and exploring data -->

<!-- Included in this template is a file called `flights.csv`.  This file includes a subset of the larger dataset of information about all flights that departed from Seattle and Portland in 2014.  More information about this dataset and its **R** package is available at <http://github.com/ismayc/pnwflights14>.  This subset includes only Portland flights and only rows that were complete with no missing values.  Merges were also done with the `airports` and `airlines` data sets in the `pnwflights14` package to get more descriptive airport and airline names. -->

<!-- We can load in this data set using the following command: -->

<!-- ```{r load_data} -->
<!-- flights <- read.csv("data/flights.csv") -->
<!-- ``` -->

<!-- The data is now stored in the data frame called `flights` in **R**.  To get a better feel for the variables included in this dataset we can use a variety of functions.  Here we can see the dimensions (rows by columns) and also the names of the columns. -->

<!-- ```{r str} -->
<!-- dim(flights) -->
<!-- names(flights) -->
<!-- ``` -->

<!-- Another good idea is to take a look at the dataset in table form.  With this dataset having more than 50,000 rows, we won't explicitly show the results of the command here.  I recommend you enter the command into the Console **_after_** you have run the **R** chunks above to load the data into **R**. -->

<!-- ```{r view_flights, eval=FALSE} -->
<!-- View(flights) -->
<!-- ``` -->

<!-- While not required, it is highly recommended you use the `dplyr` package to manipulate and summarize your data set as needed.  It uses a syntax that is easy to understand using chaining operations.  Below I've created a few examples of using `dplyr` to get information about the Portland flights in 2014.  You will also see the use of the `ggplot2` package, which produces beautiful, high-quality academic visuals. -->

<!-- We begin by checking to ensure that needed packages are installed and then we load them into our current working environment: -->

<!-- ```{r load_pkgs, message=FALSE} -->
<!-- # List of packages required for this analysis -->
<!-- pkg <- c("dplyr", "ggplot2", "knitr", "bookdown", "devtools") -->
<!-- # Check if packages are not installed and assign the -->
<!-- # names of the packages not installed to the variable new.pkg -->
<!-- new.pkg <- pkg[!(pkg %in% installed.packages())] -->
<!-- # If there are any packages in the list that aren't installed, -->
<!-- # install them -->
<!-- if (length(new.pkg)) -->
<!--   install.packages(new.pkg, repos = "http://cran.rstudio.com") -->
<!-- # Load packages (thesisdown will load all of the packages as well) -->
<!-- library(thesisdown) -->
<!-- ``` -->

<!-- \clearpage -->

<!-- The example we show here does the following: -->

<!-- - Selects only the `carrier_name` and `arr_delay` from the `flights` dataset and then assigns this subset to a new variable called `flights2`. -->

<!-- - Using `flights2`, we determine the largest arrival delay for each of the carriers. -->

<!-- ```{r max_delays} -->
<!-- flights2 <- flights %>% -->
<!--   select(carrier_name, arr_delay) -->
<!-- max_delays <- flights2 %>% -->
<!--   group_by(carrier_name) %>% -->
<!--   summarize(max_arr_delay = max(arr_delay, na.rm = TRUE)) -->
<!-- ``` -->

<!-- A useful function in the `knitr` package for making nice tables in _R Markdown_ is called `kable`.  It is much easier to use than manually entering values into a table by copying and pasting values into Excel or LaTeX.  This again goes to show how nice reproducible documents can be! (Note the use of `results="asis"`, which will produce the table instead of the code to create the table.)  The `caption.short` argument is used to include a shorter title to appear in the List of Tables. -->

<!-- ```{r maxdelays, results="asis"} -->
<!-- kable(max_delays, -->
<!--       col.names = c("Airline", "Max Arrival Delay"), -->
<!--       caption = "Maximum Delays by Airline", -->
<!--       caption.short = "Max Delays by Airline", -->
<!--       longtable = TRUE, -->
<!--       booktabs = TRUE) -->
<!-- ``` -->

<!-- The last two options make the table a little easier-to-read. -->

<!-- We can further look into the properties of the largest value here for American Airlines Inc.  To do so, we can isolate the row corresponding to the arrival delay of 1539 minutes for American in our original `flights` dataset. -->


<!-- ```{r max_props} -->
<!-- flights %>% filter(arr_delay == 1539, -->
<!--                   carrier_name == "American Airlines Inc.") %>% -->
<!--   select(-c(month, day, carrier, dest_name, hour, -->
<!--             minute, carrier_name, arr_delay)) -->
<!-- ``` -->

<!-- We see that the flight occurred on March 3rd and departed a little after 2 PM on its way to Dallas/Fort Worth.  Lastly, we show how we can visualize the arrival delay of all departing flights from Portland on March 3rd against time of departure. -->

<!-- ```{r march3plot, fig.height=3, fig.width=6} -->
<!-- flights %>% filter(month == 3, day == 3) %>% -->
<!--   ggplot(aes(x = dep_time, y = arr_delay)) + geom_point() -->
<!-- ``` -->

<!-- ## Additional resources -->

<!-- - _Markdown_ Cheatsheet - <https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet> -->

<!-- - _R Markdown_ Reference Guide - <https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf> -->

<!-- - Introduction to `dplyr` - <https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html> -->

<!-- - `ggplot2` Documentation - <http://docs.ggplot2.org/current/> -->
