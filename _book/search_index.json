[
["index.html", "Chapter 1 Introduction", " Chapter 1 Introduction The french mathematician Louis Bachelier was the trail-blazer that brought Brownian motion, previously restricted to the field of botanics where it was firstly observed, to the financial framework. He modeled the stock prices as a Brownian motion with drift. IN the 1950’s Paul Samuelson created a model based on the Geometric Brownian Motion to price options. He and others computed paths and the expected discounted payoff of European options, but with no standard discount factor or growth rate. In 1973, Black and Scholes [1] designed a way to effectively calculate a European option price that became the de facto standard modeling strategy. Options are derivatives that give their bearers the rights to buy or sell a specific asset in a future date and with a predetermined price. They are, by design, affected by variations in the underlying assets’ components, for example, the variance. The Black-Scholes model was once the standard way of option pricing, but was replaced by more recent models that are now prevalent. One of the main drawbacks of the Black-Scholes model is the strong assumption that the stock returns’ volatility is constant. Thus, the implied model’s volatility results in a flat surface when plotted against the option’s strike price and maturity. Real world implied volatility varies with the strike price and maturity, forming what is called the ‘volatility smile’. The Heston model is an extension of the Black-Scholes model that tackles this volatility issue replacing the constant volatility with a stochastic process. This model was presented by Heston in 1993 and approaches the problem introducing an efficient closed form solution to compute vanilla options, which makes this model easy to be transferred to real life pricing. Its implementations is simple when compared to its more sophisticated competitors and calibrating Heston’s model, where pricing several times the same asset is required so that parameters reproducing market prices can be obtained, are easy and can be done in a short amount of time. Furthermore, this model presents a dynamic for the underlying asset which takes into account asymmetry and kurtosis, two often observed statistical moments. We aim with this thesis to better understand the Heston model, its quirkinesses, associated numerical methods and their implementations, and eventually discuss its pros and cons. For such, we will not only discuss the challenges in a vaguely way, but show code snippets of our implementations in R programming language [2] to help clarify possible associated difficulties and approach numerical results. More specifically, we find that the exact algorithm, produces bias in practical examples, although the theoretical framework says it shouldn’t. Our test produce an estimator with a bias of \\(0.02\\), considerably smaller than the ones tested with other models (\\(0.09\\)), since we performed considerably less simulations. Furthermore, since we didn’t find code for the exact algorithm implementation we intend to release a CRAN package with the codes as a didactic tool. This thesis is divided into five chapters, the first being this introduction. Following, we have a literature review that mainly addresses stochastic calculus, the Black-Scholes models and the Heston model. Thereafter we present the different Heston model implementations. Chapter 4 brings the results of the tests we perform and finally, chapter 5 presents the conclusion. "],
["2-lt-review.html", "Chapter 2 Theoretical Framework 2.1 Stochastic Calculus 2.2 Black-Scholes Model 2.3 Stochastic Volatility models 2.4 Numerical Methods", " Chapter 2 Theoretical Framework This chapter presents the concepts of stochastic calculus, from the historic conception of how it first arose through the basic principles and applications in finance. We address with more care the classical Black-Scholes model and its limitations and the Heston model. This model is also well known, it brings the concept of stochastic volatility in it, which presents results closer to reality. 2.1 Stochastic Calculus Stochastic calculus arises from stochastic processes and allows the creation of a theory of integration where both the integrand and integrator terms are stochastic processes. Stochastic calculus was created by the Japanese mathematician Kiyosi Itô in the 1940s and 1950s and is used for modeling financial options and in another wide variety of fields [3]. In this chapter we present the historical contexts in which the tools and models are used, but our focus is introducing the concepts and notations that will be further used in our work. 2.1.1 Stochastic differential equation - SDE At first, before introducing stochastic differential equation, it is helpful to start with ordinary differential equation. Let \\(x_t\\) denote an asset at time \\(t\\) so that the change in the asset at time \\(t\\) is given by the following deterministic differential equation: \\[\\begin{align} dx_t &amp;= f(t, x_t)dt \\\\ x(0) &amp;= x_0 \\nonumber \\end{align}\\] We now add a ``noise’’ to this equation: \\[\\begin{align} \\label{eq:sde1} dx_t &amp;= \\overbrace{\\underbrace{f(t, x_t)}_\\text{drift}dt}^\\text{deterministic} + \\overbrace{\\underbrace{g(t, x_t)}_\\text{diffusion}dW_t}^\\text{random shock} \\\\ x(0) &amp;= x_0 \\nonumber \\end{align}\\] This ``noise’’ \\(W_t\\) is a random Wiener process (which will be clarified below) and \\(x_0\\) is our initial value. The \\(g(t, x_t)\\) part of the SDE is often referred as diffusion coefficient. Before moving on, we must carefully define what the term random means and the best way to begin doing so is to precisely define a probability space: Definition 2.1 (Probability Space) A triple (\\(\\Omega\\), \\(\\mathcal {U}\\), \\(\\mathcal {P}\\)) is called a provided \\(\\Omega\\) is any set, \\(\\mathcal {U}\\) is a \\(\\sigma\\)-algebra of subsets of \\(\\Omega\\) and \\(\\mathcal {P}\\) is a probability measure on \\(\\mathcal {U}\\) . 2.1.2 Brownian Motion The Brownian motion is the name given to the irregular motion observed in the motion of pollen particles suspended in fluid resulting from particle collision with atoms or molecules. It is named after Robert Brown, the first to have observed the movement in 1828. He noted two characteristic in the pollen movement [4]: the path of a given particle is very irregular, having a tangent at no point the motion of two distinct particles appear to be independent The first quantitative works in Brownian motion come from an interest in stock price fluctuation by Bachelier in 1900. Albert Einstein also leaned over the subject and in 1905 derived the transition density for Brownian motion from molecular-kinetic theory of heat [3,5]. In 1923, the Wiener process was coined in honor of Norbert Wiener mathematical proof of existence of the Brownian motion and stating its properties.1 Definition 2.2 (Wiener Process) Given a probability space (\\(\\Omega\\), \\(\\mathcal {U}\\), \\(\\mathcal {P}\\)), a stochastic process \\(W_t\\) defined in this space is a Wiener process if it satisfies the following properties: Furthermore, if a process is a Wiener process, we shall have the following results by construction: Figure 2.1: A Wiener process trajectory path example 2.1.2.1 Correlated Brownian Motions Two independent Brownian motions that are correlated can describe a new process \\(Z_t\\). Let \\(W_1\\) and \\(W_2\\) be these two independent Brownian motions and let \\(-1 \\leq \\rho \\leq 1\\) be a given number. For \\(0 \\leq t \\leq T\\) define the new process \\(Z_t\\) as [3]: \\[\\begin{align} \\label{eq:corr_brow} Z_t = \\rho W_{1,t} + \\sqrt{1-\\rho^2}W_{2,t} \\end{align}\\] This equation is a linear combination of independent normals at each timestep \\(t\\), so \\(Z_t\\) is normally distributed. It is proven that \\(Z\\) is a Brownian motion and that \\(Z\\) and \\(W_{1,t}\\) have correlation \\(\\rho\\) [3]. 2.1.3 Itô’s Integral Formally, the SDE presented in equation (??) is interpreted as [7–12]: \\[\\begin{align} \\label{eq:sde2} x_t &amp;= x_0 + \\int_{0}^{t}{f(s, x_s)ds} + \\int_{0}^{t}{g(s, x_s)dW_s} \\end{align}\\] for some \\(f\\), \\(g\\) and \\(s \\in [0,t]\\). 2.1.3.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.1 The Itô integral can, as the Riemann integral, be approximated by a finite sum . Also, it has a definition as a certain limit. Itô’s lemma 2.1 plays the same role as the fundamental theorem of calculus in allowing to evaluate integrals. It presents an extra term not encountered in the conventional calculus theorem that is due to the non-smoothness characteristics of Brownian motion paths. It is possible, though, to define the integral as the summation below: \\[\\begin{align} Y_{\\Delta t}(t) = \\sum_{t_k &lt; t}{g(t_k)\\Delta W_k} \\end{align}\\] with the usual notions \\(t_k = k\\Delta t\\), and \\(\\Delta W_k = W(t_{k+1})-W(t_k)\\). And in a more rigorous form, if the limit exists, then the Ito integral is: \\[\\begin{align} Y(t) = \\lim\\limits_{\\Delta t \\to 0} Y_{\\Delta t}(t) \\end{align}\\] It is essential that the forward difference, that is \\(W(t_{k+1})-W(t_k)\\) is used rather than the backward difference \\(W(t_{k})-W(t_{k-1})\\), which would be wrong. Lemma 2.1 (Itô’s Lemma) Assume that \\(S_t\\) has a stochastic differential given by: \\[\\begin{align} dS_t = \\mu_t dt + \\sigma_t dW_t \\end{align}\\] for \\(\\mu_t\\), \\(\\sigma_t\\) and \\(t \\in [0,T]\\) and that \\(\\mathbb{C}^{2,1} \\left( \\mathbb{R} \\times \\left[0, T \\right]\\right)\\). Assume \\(u: \\mathbb{R} \\times [0, T] \\rightarrow \\mathbb{R}\\) is continuous and that \\(\\frac{\\partial u}{\\partial t}\\), \\(\\frac{\\partial u}{\\partial x}\\), \\(\\frac{\\partial^2 u}{\\partial x^2}\\) exist and are continuous. \\[\\begin{align*} Y_t := u(S_t, t) \\end{align*}\\] Then Y has the following stochastic differential: \\[\\begin{align} \\label{eq:ito} \\begin{split} dY_t &amp;= \\frac{\\partial u}{\\partial t}dt + \\frac{\\partial u}{\\partial x} dS_t + \\frac{1}{2}\\frac{\\partial^2 u}{\\partial x^2}\\sigma_t^2 dt \\\\[10pt] &amp;= \\left( \\frac{\\partial u}{\\partial t} + \\mu_t \\frac{\\partial u}{\\partial x} + \\frac{1}{2}\\frac{\\partial^2 u}{\\partial x^2}\\sigma_t^2 \\right) dt + \\sigma_t \\frac{\\partial u}{\\partial x} dW_t \\end{split} \\end{align}\\] where the argument of \\(\\frac{\\partial u}{\\partial t}\\), \\(\\frac{\\partial u}{\\partial x}\\) and \\(\\frac{\\partial^2 u}{\\partial x^2}\\) above is \\(\\left( S_t, t \\right)\\) . Equation (??) is the stochastic equivalent to the chain rule, also known as Itô’s formula or Itô’s chain rule. The proof to this theorem is based on the Taylor expansion of the function \\(f(S_t, t)\\) [7,9]. For practical use you should write out a second-order Taylor expansion for the function to be analyzed and apply the multiplication table [3] presented in Table ??. 2.1.3.1 Itô’s Integral Properties Let \\(f\\), \\(g\\) \\(\\in\\) \\(\\mathcal{V}\\) and let \\(0 \\leq t_0 &lt; u &lt; T\\). Then 2.2 Black-Scholes Model 2.2.1 Basics The Black-Scholes (B-S) model arises from the need to price european options in the derivative markets. Derivatives are financial instruments traded in the market, stock exchange or over-the-counter (OTC) market, whose values depend on the values of an underlying asset. [1,13,14] A call option is a derivative that gives its bearer the right, but not the obligation, to purchase a specific asset by a fixed price before or on a given date. A put option is a derivative that gives its bearer the right, but not the obligation, to sell a specific asset by a fixed price before or on a given date. The trading price of the option is called the option premium and the asset from which the option derives is called the underlying asset. This asset may be interest rates, exchange rates, stock exchanges indices, commodities or stocks. The fixed price in contract in which the underlying asset might to be bought or sold is the strick price. The option expiration date is called the maturity. [1,14] There are two major different option types: European and American. The difference between these two is that the bearer of the first may exercise it only at the end of its life, at its maturity while the latter can be exercised at any given time until its maturity. [1,15] 2.2.1.1 Geometric Brownian Motion A stochastic process \\(S_t\\) is a Geometric Brownian Motion2 if it is described by the solution of the following stochastic differential equation [3,9,16]. \\[\\begin{align} dS_t = \\mu S_t dt + \\sigma S_t dW_t \\end{align}\\] for given constants \\(\\mu \\in {\\rm I\\!R}\\) and \\(\\sigma &gt; 0\\). Also, the assumed initial value is positive, \\(S_0 &gt;0\\). Figure 2.2 shows the GBM,3 which is quite often applied to model the dynamics of some financial assets because of its properties [17]. Equation (??) shows the formula to generate a GBM and we provide proof of this solution in appendix A4 \\[\\begin{align} \\label{eq:sde3} S_t = S_0 \\times exp{\\left( \\left( \\mu - \\frac{\\sigma^2}{2} \\right) t + \\sigma W_t \\right)}, \\;\\; t &gt; 0 \\end{align}\\] Figure 2.2: GBM trajectories path example The Black-Scholes model provides analytical solution to the price of a European call at time \\(t\\) and can be described as follows [1,6,13]: \\[\\begin{align} C(S_{t},t)&amp;=N(d_{1})S_{t}-N(d_{2})Ke^{-r(T-t)}\\\\[10pt] d_{1}&amp;={\\frac {1}{\\sigma {\\sqrt {T-t}}}}\\left[\\ln \\left({\\frac {S_{t}}{K}}\\right)+\\left(r+{\\frac {\\sigma ^{2}}{2}}\\right)(T-t)\\right]\\\\[10pt] d_{2}&amp;=d_{1}-\\sigma {\\sqrt {T-t}} \\end{align}\\] Where: \\(S_{t}\\) is the spot price of the underlying asset at time \\(t\\) \\(r\\) is the risk free rate (generally an annual rate) \\(\\sigma\\) is the volatility of returns of the underlying asset \\(N(\\cdot )\\) is the cumulative distribution function of the standard Gaussian distribution \\(K\\) is the strike price \\(T-t\\) is the time to maturity Also, the stock price path is a Geometric Brownian Motion as previously stated, and is under the risk-neutral measure with the following dynamics [6,19]: \\[\\begin{align} dS_{t} = (r-q)S_td_t+\\sigma S_t dW_t \\end{align}\\] Here \\(W_t\\) is a Wiener process [1,19], \\(r\\) is the risk free rate and \\(q\\) is the dividend yield and \\(t\\) denotes the current point in time. 2.2.2 Limitations Although the Black-Scholes is very popular and the de facto standard in the market there are implications to the B-S model assumptions that affect the results and that are unrealistic. The main assumption that does not hold up is the deterministic (constant) volatility, that can more accurately be described as a stochastic process since we observe that small moves usually are followed by small moves and large moves by large moves. [6,13] Other assumptions that are critical to the B-S model and are not always observed in practice refer to the asset’s continuity through time (no jumps), being allowed to perform continuous hedge without transactions costs and normal (Gaussian) returns. Most models focus on the volatility problem because transaction costs often implies rises in volatility and that fat-tails (abnormal) returns can be simulated by stochastic volatility and market or volatility jumps. 2.3 Stochastic Volatility models Introducing stochastic volatility to models brings complexity, but enables modeling some features observed in reality that are crucial, like the randomic market volatility effects, skewness (market returns are more realistically modeled) and volatility smile5 (see Figure 2.3). This kind of model is applied highly succesfully in foreign exchange and credit markets. Definition 2.3 (Volatility Smile) Volatility smiles are implied volatility patterns that arise in pricing financial options. In particular for a given expiration, options whose strike price differs substantially from the underlying asset’s price command higher prices (and thus implied volatilities) than what is suggested by standard option pricing models. These options are said to be either deep in-the-money or out-of-the-money. Furthermore, stochastic volatility models use statistical methods as foundations to price and forecast options’ behavior. The Heston, the \\(3/2\\) and other models, like the GARCH6 and SABR,7 are considered standard smile models. Figure 2.3: Volatility Smile 2.3.1 Cox-Ingersoll-Ross model The Cox-Ingersoll-Ross (CIR) model is a well-known short-rate model that describes the interest rate movements driven by one source of market risk. The dynamics are described as follows [23,24]: \\[\\begin{align} \\label{eq:cir} dr_t &amp;= \\kappa (\\theta - r_t)dt + \\sigma \\sqrt{r_t} dB_t \\end{align}\\] Where, \\(r_t\\) is the short rate interest described by parameters \\(\\kappa\\) the speed of mean reversion, \\(\\theta\\) the long-run mean and \\(\\sigma\\) the volatility process. This model has been widely used to describe the dynamics of the short rate interest because it has some fundamental features like intuitive parametrization, nonnegativity and pricing formulas. Besides, it takes account of anticipations, risk aversion, investment alternatives and preferences about consumption timing and allows for detailed predictions about how changes in a wide range of underlying variables affect the term structure[23]. Furthermore, this equation constitutes one of the two Heston model equations with the volatility taking the short rate interest place. 2.3.2 Heston Model Heston model was introduced in 1993 by Steven Heston to solve the deterministic volatility problems. It was designed to analize bond and currency options and it introduced the following equations, which represent the dynamics of the stock price and the variance processes under the risk-neutral measure [24,25]: \\[\\begin{align} \\label{eq:heston} dS_t &amp;= \\mu S_t dt + \\sqrt{V_t} S_t dW_t \\\\ dV_t &amp;= \\kappa (\\theta - V_t)dt + \\sigma \\sqrt{V_t} dB_t \\label{eq:hesvar} \\end{align}\\] The second equation, as described in Section 2.3.1, is the CIR model equation. The first equation states the asset price process. \\(\\mu\\) is the asset’s rate of return, \\(dW_{t}\\) and \\(dB_{t}\\) are two correlated wiener processes with correlation coefficient of \\(\\rho\\). The parameters \\(\\kappa\\), \\(\\theta\\) and \\(\\sigma\\) represent the speed of mean reversion, the long run mean variance and the volatility of the variance (vol of vol), respectively. These parameters are analogous to the ones in the CIR model. 2.4 Numerical Methods Numerical methods are tools that are often applied to solve stochastic differential equations because most of these do not have explicit solution. This means that we are not able to solve these equations using symbolic computation. Although we are unable to find an analitical solution, when facing real problems, the approximation given by a numerical method is often sufficient. Alongside the analytical issue, the need to calculate the SDE’s trajectory through time is the main reason why studying numerical methods is so important. An implementation of a numerical method is called a numerical algorithm. We will simulate sample paths of time discrete approximations implemented in the R programming language [2] that we base on a finite discretization of a time interval \\([t_0, T]\\). We shall generate approximate values of the sample path for each step contained in the discretized interval [26]. In the fixed step methods, the distance between two contiguous points is \\(d_i = t_i - t_{i-1} = \\frac{T-t_0}{N} \\;\\;\\; i \\leq N \\in \\mathbb{N}\\). \\(N\\) being the time interval partition number. According to Kloeden [26], in the stochastic realm, simulated sample paths can be statistically analysed to find how good an approximation is compared to an exact solution. Moreover, the computational costs such as time and memory increases polynomially with the problem’s dimension, which is good, and it is possible to apply variance reduction methods that allow a considerable decrease in the required sample size. 2.4.1 Convergence As soon as we talk about numerical methods we are required to approach the topic of approximations and how to handle them. Methods efficiency receive the name of convergence order. In the SDE domain there are two main methods of convergence, that are classified according to their criteria. Firstly, we present the strong order of convergence. A method is said to have strong convergence \\(\\delta\\) to \\(Y\\) if a time discretized \\(Y_{\\delta}\\) of a continous-time process \\(Y\\), with \\(\\delta\\) being the maximum time increment of the discretization, and for any fixed time horizon \\(T\\) holds true that [17]: \\[\\begin{align*} \\mathbb{E} \\mid Y_{\\delta}(T) - Y(T) \\mid \\leq C \\delta^{\\gamma}, \\,\\,\\, \\forall \\delta &lt; \\delta_0, \\end{align*}\\] with \\(\\delta_0 &gt; 0\\) and \\(C\\) a constant not depending on \\(\\delta\\). Strong convergence addresses the problem of solutions’ trajectories. For specific conditions, the Euler method has strong convergence order \\(\\gamma = \\frac{1}{2}\\). Furthermore, there is the weak order of convergence: \\[\\begin{align*} \\mid \\mathbb{E}p(Y(T)) - \\mathbb{E}p(Y_\\delta(\\tau)) \\mid \\leq C \\delta t^{\\gamma}, \\,\\,\\, \\forall \\delta &lt; \\delta_0, \\end{align*}\\] Strong and weak convergence are not mutually exclusive [17]. That means that a method with a given strong order of convergence might have a higher weak order of convergence too. This is the case for the Euler scheme, with a strong order of convergence of \\(1/2\\) and a weak order of \\(1\\) (under some conditions). \\(p\\) is a continuous differentiable function of polynomial growth. For a more detailed and rigorous explanation of convergence we recommend consulting [27]. It is worth noting that, altough schemes have a given convergence order, it is not unusual that they behave better than their order for some SDEs specifications. 2.4.2 Discretization We know that convergence is an important feature to a numerical method and studies have found not all time discrete possible approximations of an SDE converge in a useful sense to the solution process as the step size adopted tends toward zero [28,29]. Moreover, particularly for SDEs, some of the more rapidly convergent methods available for ordinary differential equations (ODE) do not work, such as higher order Runge-Kutta methods.8 One of the methods that do work for ODEs and SDEs is the Euler method, named after the Swiss mathematician Leonhard Euler. Figure 2.4 shows an example of an implementation for the Newton’s cooling law with timestep of 2 seconds compared to its analytical solution. This method (a.k.a. forward Euler method) is a first-order numerical procedure. It is the most basic explicit method9 for numerical integration. Figure 2.4: Analytical x Euler solutions The method is first-order, as stated above, this means that the error in each step is a proportion of the square of the step size. Also, the global error at a given time is a function of the step size. We proceed to apply the Euler method to SDEs. Consider the equation: \\[\\begin{align} dS_t &amp;= \\mu(S_t,t) dt + \\sigma(S_t,t) dW_t \\end{align}\\] \\(dW_t\\) is the Brownian motion, \\(\\mu\\) and \\(\\sigma\\) are functions depending on \\(S_t\\) and \\(t\\), over an interval \\([0,T]\\), and we want to discretize it as \\(0 = t_1 &lt; t2 &lt; \\cdots &lt; t_m = T\\) with increments equally spaced \\(\\Delta t\\). Integrating it from \\(t\\) to \\(\\Delta t\\) we have the starting point for our (and any) discretization scheme: \\[\\begin{align} \\label{eq:disc1} S_{t+\\Delta t} &amp;= S_t + \\int_{t}^{\\Delta t}{\\mu(S_u,u)}du + \\int_{t}^{\\Delta t}{\\sigma(S_u,u)} dW_u \\end{align}\\] To use the Euler discretization is the equivalent of approximating integrals using the left-point rule as in Figure 10, we then have: \\[\\begin{align*} \\int_{t}^{t+\\Delta t}{\\mu(S_u,u)} dW_u &amp;\\approx \\mu(S_t,t) \\int_{t}^{t+\\Delta t}dW_u\\\\ &amp;= \\mu(S_t,t) (W_{t+\\Delta t} - W_t) \\\\ \\int_{t}^{t+\\Delta t}{\\sigma(S_u,u)} dW_u &amp;\\approx \\sigma(S_t,t) \\int_{t}^{t+\\Delta t}dW_u = \\sigma(S_t,t) (W_{t+\\Delta t} - W_t) \\\\ &amp;= \\sigma(S_t,t) \\sqrt{\\Delta t} Z \\end{align*}\\] \\(W_{t+\\Delta t}-W_t\\) and \\(\\sqrt{\\Delta t}Z\\) have identical distribution, \\(Z\\) being a standard Gaussian variable. The Euler discretization of equation (??) is then: \\[\\begin{align} \\label{eq:disc2} S_{t+\\Delta t} &amp;= S_t + \\mu(S_t,t)\\Delta t + \\sigma(S_t,t)\\sqrt{\\Delta t}Z \\end{align}\\] 2.4.2.1 Euler method - Heston model We now proceed to apply the method to our model of interest. We retake the equations (??) and (??). We begin showing how to discretize the latter [17,27]: \\[\\begin{align} \\label{eq:hesvareuler} V_{t+\\Delta t} = V_t+ \\int_{t}^{t+\\Delta t}{\\kappa (\\theta - V_u) du} + \\int_{t}^{t+\\Delta t}{\\sigma \\sqrt{V_u} dB_u} \\end{align}\\] Which discretized turns out as: \\[\\begin{align*} \\int_{t}^{t+\\Delta t}{\\kappa (\\theta - V_u)} du &amp;\\approx \\kappa (\\theta - V_t) \\Delta t\\\\ \\int_{t}^{t+\\Delta t}{\\sigma \\sqrt{V_u}} dB_t &amp;\\approx \\sigma \\sqrt{V_t} (W_{t+\\Delta t}-W_t) = \\sigma \\sqrt{V_t \\Delta t} Z_v \\end{align*}\\] And leaves us with: \\[\\begin{align} V_{t+\\Delta t} = V_t + \\kappa (\\theta - V_t) \\Delta t + \\sigma \\sqrt{V_t \\Delta t} Z_v \\end{align}\\] \\(Z_v\\) is a standard normal variable. To avoid problems with negative values in \\(\\sqrt{V_t}\\) we apply the full truncation scheme, which substitutes \\(V_t\\) with \\(V_t^+ = max(0, V_t)\\).11 For the \\(S_t\\) SDE we proceed similarly: \\[\\begin{align} \\label{eq:heseuler} S_{t+\\Delta t} = S_t+ \\mu \\int_{t}^{t+\\Delta t}{ S_u du} + \\int_{t}^{t+\\Delta t}{\\sqrt{V_u} S_u dW_u} \\end{align}\\] Discretizing we have: \\[\\begin{align*} \\int_{t}^{t+\\Delta t}{S_u} du &amp;\\approx S_t \\Delta t\\\\ \\int_{t}^{t+\\Delta t}{\\sqrt{V_u} S_u} dW_u &amp;\\approx \\sqrt{V_t} S_t (W_{t+\\Delta t}-W_t)\\\\ &amp;= \\sqrt{V_t \\Delta t} S_t Z_s \\end{align*}\\] \\(Z_s\\) is a standard normal variable with correlation \\(\\rho\\) with \\(Z_v\\). We have: \\[\\begin{align} S_{t+\\Delta t} = S_t + \\mu S_t \\Delta t + \\sqrt{V_t \\Delta t} S_t Z_s \\end{align}\\] More can be found on [6–8].↩ There is an Arithmetic Brownian Motion: \\(dS_t = \\mu dt + \\sigma dB_t\\). More information can be obtained in [3].↩ Also known as exponential Brownian motion.↩ An intuitive proof can be found at [18].↩ The name derives from the concave shape of the graph, which resembles a smile.↩ generalized autoregressive conditional heteroscedasticity. [20]↩ stochastic alpha, beta, rho. [22]↩ The euler method is the simplest Runge-Kutta method.↩ Explicit methods calculate the state of a system at a later time from the state of the system at the current time. Mathematically we have something like \\(Y(t+\\Delta t)=F(Y(t))\\,\\).↩ See Kiusalaas [30]↩ Another possible scheme (not used in this work) is the reflection scheme where we replace \\(V_t\\) with \\(\\mid V_t \\mid\\)↩ "],
["3-implement.html", "Chapter 3 The Heston Model Implementation 3.1 Characteristic Function 3.2 Euler Scheme 3.3 Kahl-Jackel 3.4 Exact Algorithm", " Chapter 3 The Heston Model Implementation In section 2.3.2 we presented Heston’s SDE system in one of its structures. Another common way [31–33] to write down the system is using the property presented in subsection 2.1.2.1 as in equation (??). \\[\\begin{align} \\label{eq:heston2} \\begin{split} dS_t &amp;= r S_t dt + \\rho \\sqrt{V_t} S_t dB_t + \\sqrt{1 - \\rho^2} \\sqrt{V_t} S_t dW_t \\\\ dV_t &amp;= k(\\theta - V_t)dt + \\sigma \\sqrt{V_t} dB_t \\end{split} \\end{align}\\] 3.1 Characteristic Function The Heston model characteristic function is firstly presented in the 1993 Steven Heston’s paper [24] and is described below [34]: \\[\\begin{align} f(S_t, V_t, t) = e^{A(T-t)+B(T-t)S_t + C(T-t)V_t + i \\phi S_t} \\end{align}\\] If we let \\(\\tau = T-t\\), then the explicit form of the Heston characteristic function is: \\[\\begin{align*} f(i \\phi) &amp;= e^{A(\\tau)+B(\\tau)S_t + C(\\tau)V_t + i \\phi S_t} \\\\ A(\\tau) &amp;= r i \\phi \\tau + \\frac{\\kappa \\theta}{\\sigma^2} \\left[ - (\\rho \\sigma i \\phi - \\kappa - M) \\tau - 2 \\ln\\left(\\frac{1-N e^{M \\tau}}{1-N}\\right) \\right] \\\\ B(\\tau) &amp;= 0 \\\\ C(\\tau) &amp;= \\frac{(e^{M \\tau}-1)(\\rho \\sigma i \\phi - \\kappa - M)}{\\sigma^2 (1-N e^{M \\tau})} \\\\ \\text{Where:} &amp; \\\\ M &amp;= \\sqrt{(\\rho \\sigma i \\phi - \\kappa)^2 + \\sigma^2 (i \\phi + \\phi^2)} \\\\ N &amp;= \\frac{\\rho \\sigma i \\phi - \\kappa - M}{\\rho \\sigma i \\phi - \\kappa + M} \\\\ \\end{align*}\\] This function is the driving force behind the following formula, that calculates the fair value of a European call option at time \\(t\\), given a strike price \\(K\\), that expires at time \\(T\\) [34]: \\[\\begin{align} \\label{eq:cfheston} \\begin{split} C = &amp; \\frac{1}{2} S(t) + \\frac{e^{-r(T-t)}}{\\pi}\\int_{0}^{\\infty}{\\Re \\left[ \\frac{K^{-i \\phi} f(i \\phi + 1)}{i \\phi} \\right] d\\phi} \\\\ &amp; -Ke^{-r(T-t)}\\left( \\frac{1}{2} + \\frac{1}{\\pi} \\int_{0}^{\\infty}{\\Re \\left[ \\frac{K^{-i \\phi} f(i \\phi)}{i \\phi} \\right]} d\\phi \\right) \\end{split} \\end{align}\\] 3.2 Euler Scheme Given the fact that the underlying asset is temporal dependent upon the solution of the SDE’s volatility, we simulate the volatility’s path before the asset’s. If the Black-Scholes model enabled using Ito’s Lemma directly for solving \\(S_t\\), this equation system requires numerical methods. We present here the Euler Scheme - Full Truncation algorithm (and compare to other similar schemes) [31] along with some insights on how it was implemented in R. The Euler discretization brings approximation paths to stock prices and variance processes. If we set \\(t_0 = 0 &lt; t_1 &lt; \\dots &lt; t_M = T\\) as partitions of a time interval of \\(M\\) equal segments of length \\(\\delta t\\), we have the following discretization for the stock price: \\[\\begin{align} S_{t + \\Delta t} = S_t + rS_t dt + \\sqrt{V_t} S_t Z_s \\end{align}\\] And for the variance process: \\[\\begin{align} V_{t + \\Delta t} = f_1(V_{t}) + \\kappa (\\theta - f_2(V_{t})) + \\sigma \\sqrt{f_3(V_{t})} Z_v \\end{align}\\] \\(Z_s\\) being a standard normal random variable, i.e. \\(N\\sim(0,1)\\), we set \\(Z_t\\) and \\(Z_v\\) as two independent standard normal random variables and \\(Z_s\\) and \\(Z_v\\) having correlation \\(\\rho\\). This means we can write \\(Z_s = \\rho Z_v + \\sqrt{1-\\rho^2} Z_t\\). \\(r\\) is the risk free interest rate. The immediate observable problem in the proposed discretization scheme is that \\(V\\) can become negative with non-zero probability making the computation of \\(\\sqrt{V_t}\\) impossible [32]. There are several proposed fixes that can be used as you can see below: Where \\(V^+ = \\max(V,0)\\) and \\(\\mid V \\mid\\) is the absolute value of \\(V\\). We chose to fix our discretization using the Full-Truncation (FT) scheme and thus, rewrite the equations as follows: \\[\\begin{align} \\label{eq:st1} S_{t\\Delta} &amp;= S_t + rS_t + \\sqrt{V_{t}^{+}} S_t Z_s \\\\ \\label{eq:vt1} V_{t\\Delta} &amp;= V_t + \\kappa (\\theta - V_{t}^{+}) + \\sigma \\sqrt{V_{t}^{+}} Z_v \\end{align}\\] Our R implementation follows the euler’s scheme with hardly any modifications. It draws two Gaussian random variables (\\(Z_v\\) and \\(Z_t\\)) using the function rnorm to create \\(Z_s\\) with correlation \\(\\rho\\) with \\(Z_v\\). Zv &lt;- stats::rnorm(N) Zt &lt;- stats::rnorm(N) Zs &lt;- rho * Zv + (sqrt(1 - (rho^2)) * Zt) And use it to compute \\(S\\) and \\(V\\) as in the code snippet bellow. The two modifications we apply are previously computing the square roots of the stepsize dt and the aux variable as to improve speed. The aux variable is an help variable created to impose positivity to \\(V\\) as we are operating the full-truncation euler scheme. S &lt;- S * (1 + r * dt + sqrt_aux * Zs * sqrt_dt) v &lt;- v + k * dt * (theta - aux) + sigma * sqrt_aux * Zv * sqrt_dt We could have used the pmax function in R, but this function is slow, therefore we opted to create the aux variable and impose positive values using the following R syntax: aux &lt;- v aux[v &lt; 0] &lt;- 0 Depending on the reader’s R fluency, other parts of the scheme might present a challenge, and that is why a version of the function is fully presented in appendix C.1. 3.3 Kahl-Jackel Kahl-Jackel propose a discretization method they refer to as the ``IJK’’ method [32,33] that coupled with the implicit Milstein scheme for the variance lands the system of equations (??) and (??). It is possible to verify that this discretization always results in positive paths for \\(V\\) if \\(4 \\kappa \\theta &gt; \\sigma^2\\). Unfortunately, this inequality is rarely satisfied when we plug real market data to calibrate the parameters. This means we must have a defined strategy for when the inequality doesn’t hold. We use the scheme proposed in Andersen [32], where a truncation similar to the Euler’s is applied. Whenever our volatility \\(V_t\\) drops below zero we use (??), and implement \\(V_{t+\\Delta}^{+}\\) and \\(V_t^{+}\\) instead of \\(V_{t+\\Delta}\\) and \\(V_t\\) of equation (??). The code guidance to this method can be found in appendix C.2. 3.4 Exact Algorithm In 2006, Broadie-Kaya [31] propose a method that has a faster convergence rate, \\(\\mathcal{O} \\left( s^{-1/2} \\right)\\) than some of the more famous schemes, such as Euler’s and Milstein’s discretizations. They build their idea to generate an exact sample from the distribution of the terminal stock price based on numerous papers [24]. The stock price and variance are as follows: \\[\\begin{align} \\label{eq:ea1} S_t = S_0 \\, exp \\left[ r t - \\frac{1}{2} \\int_{0}^{t}{V_s ds} + \\rho \\int_{0}^{t}{\\sqrt{V_s d B_s}} + \\sqrt{1 - \\rho^2} \\int_{0}^{t}{\\sqrt{V_s} dW_s}\\right] \\end{align}\\] The squared volatility of the variance process is: \\[\\begin{align} \\label{eq:ea2} V_t = V_0 + \\kappa \\theta t - \\kappa \\int_{0}^{t}{V_s ds} + \\sigma \\int_{0}^{t}{\\sqrt{V_s} dB_s} \\end{align}\\] The algorithm used to generate the model consists in four steps as follows: 3.4.1 Generate a sample of \\(V_t\\) given \\(V_0\\) The distribution of \\(V_t\\) given \\(V_0\\) for \\(0 &lt; t\\) is a non-central chi-squared distribution [23,38]: \\[V_t = \\frac{\\sigma^2 (1-e^{- \\kappa t})}{4 \\kappa} \\chi_{\\delta}^{2} \\left( \\frac{4 \\kappa e^{- \\kappa t}}{\\sigma^2 (1- e^{- \\kappa t})} \\times V_0\\right)\\] where \\(\\delta = \\frac{4 \\theta \\kappa}{\\sigma^2}\\) and \\(\\chi_{\\delta}^{2}(\\lambda)\\) denotes a non-central chi-squared random variable with \\(\\delta\\) degrees of freedom and \\(\\lambda\\) as its non-centrality parameter. Broadie and Kaya [31] sample generating Poisson and gamma distributions as in Johnson et al. [39]. We used the built-in function in R rchisq, which uses this exact method for sampling, see chunk below. d1 &lt;- (4 * k * theta)/(sigma)^2 c0 &lt;- (sigma^2 * (1 - exp(-k*tau)))/(4*k) dt &lt;- (tau-t) # sampling V lambda &lt;- (4*k*exp(-k*dt)*v)/(sigma^2 * (1-exp(-k*dt))) vt &lt;- c0 * stats::rchisq(n = 1, df = d1, ncp = lambda) 3.4.2 Generate a sample of \\(\\int_0^t V_sds\\) given \\(V_t\\), \\(V_0\\) After generating \\(V_t\\), we proceed to the algorithm’s next step following [31,39]. We use the characteristic function (??) to compute the probability density function \\(F(x)\\). \\[\\begin{align} \\label{eq:phi} \\begin{split} \\Phi(a) &amp;= \\mathbb{E}\\left[ exp \\left( ia \\int_{0}^{t}{V_s ds} \\mid V_0,V_t \\right) \\right] \\\\[10pt] &amp;= \\frac{\\gamma(a)e^{(-1/2)(\\gamma(a)- \\kappa) t} (1 - e^{- \\kappa t})}{\\kappa (1 - e^{- \\gamma(a) t})} \\\\[10pt] &amp;\\times exp \\left\\{\\frac{V_0 + V_t}{\\sigma^2} \\left[ \\frac{\\kappa (1 + e^{- \\kappa t})}{1 - e^{- \\kappa t}} - \\frac{\\gamma(a) (1 + e^{- \\gamma(a) t})}{1 - e^{- \\gamma(a) t}} \\right] \\right\\} \\\\[10pt] &amp;\\times \\frac{I_{0.5\\delta - 1} \\left[ \\sqrt{V_0 V_t} \\frac{4 \\gamma(a) e^{-0.5 \\gamma(a) t}}{\\sigma^2 (1 - e^{- \\gamma(a) t})} \\right]}{I_{0.5\\delta - 1} \\left[ \\sqrt{V_0 V_t} \\frac{4 \\kappa e^{-0.5 \\kappa t}}{\\sigma^2 (1 - e^{- \\kappa t})} \\right]} \\end{split} \\end{align}\\] where \\(\\gamma(a) = \\sqrt{\\kappa^2 - 2 \\sigma^2 i a}\\), \\(\\delta\\) was previously defined and \\(I_v(x)\\) is the modified Bessel function of the first kind.12 There are no mysteries implementing the characteristic function as you can observe in the chunk below. Although R has a built-in Bessel function, it only accounts for real numbers. Thus, we were obliged to use the Bessel package [40] that accounts for complex numbers. Once again, we pre compute some of the operations that are repeated through the function as to reduce computational time. This is specially important for this method since it involves operations of high complexity. phi_heston &lt;- function(a, v0, v_t, d){ gamma_a &lt;- sqrt(k^2 - 2 * sigma^2 * 1i*a) gammadt &lt;- gamma_a * (tau-t) sqrtv0vt &lt;- sqrt(v0*v_t) delta &lt;- -k * (tau-t) part1 &lt;- (gamma_a * exp(-(gamma_a - k)/2 * (tau-t)) * (1 - exp(delta)))/ (k * (1- exp(- gammadt))) part2 &lt;- exp((v0+v_t)/(sigma^2) * ( (k * (1 + exp(delta)))/(1-exp(delta)) - (gamma_a * (1 + exp(- gammadt)))/(1-exp(- gammadt)))) part3 &lt;- Bessel::BesselI(z = ((4 * gamma_a * sqrtv0vt)/(sigma^2) * exp(- gammadt/2)/ (1 - exp(- gammadt))), nu = 0.5*d - 1) / Bessel::BesselI(z = ((4 * k * sqrtv0vt)/(sigma^2) * (exp(delta/2))/ (1-exp(delta))), nu = 0.5*d - 1) return (part1 * part2 * part3) } The probability distribution function is obtained in [31,38] by Fourier inversions using Feller [41]. We use the approach in Gil-Pelaez [42], equation (??). We define \\(V(u,t)\\) the random variable with the same distribution as the integral \\(\\int_{u}^{t}{V_s ds}\\), conditional on \\(V_u\\) and \\(V_t\\): \\[\\begin{align} \\label{eq:fourier} F(x) \\equiv Pr \\left\\{ V(u,t) \\leq x \\right\\} = F_{X}(x)={\\frac {1}{2}}-{\\frac {1}{\\pi }}\\int _{0}^{\\infty }{\\frac {\\operatorname {Im} [e^{-iux} phi (u)]}{u}}\\,du \\end{align}\\] \\(\\operatorname {Im}\\) denotes the imaginary part of \\(e^{-iux} phi (u)\\). Equation (??) is computed numerically and we then sample it by inversion. The integral function of the volatility is composed by five other functions inside of it. Since it acts only as a wrapper to theses functions we are going to omit it below, but the complete code can be found in appendix C.3.The first represents our integrand. The following function takes our integrand and actually performs the integration, returning the value. For speed purposes, we limited the integral upper bound to 1000 and increased the tolerance to \\(10^{-3}\\). After these calculations we have now the function’s cumulative function. integrand &lt;- function(x, phi = cf){ f2 &lt;- function(u){ Im(phi(u) * exp(-1i * u * x)) /u } return(f2) } ## integrate to &quot;cdf&quot; F_x &lt;- function (x) { y &lt;- 0.5 - 1/pi * stats::integrate(integrand(x), lower= 0, upper= 1000, rel.tol = 0.001, stop.on.error = FALSE)$value return(y) } ## endsign endsign &lt;- function(f, sign = 1) { b &lt;- sign while (sign * f(b) &lt; 0) b &lt;- 10 * b return(b) } After computing the integral, we need to set the inversion function to place. To do that we generate the endsign function above. It is a simple function that is used to guarantee that when we call uniroot, the function to find roots built-in in R, our bounds have different signs. We set our lower and upper bounds as \\(- \\infty\\) and \\(\\infty\\), respectively and define and auxiliary function subcdf that will perform the inversion subtracting an uniform from it. ## inversion low_bound = -Inf upp_bound = Inf invcdf &lt;- function(u) { subcdf &lt;- function(t) F_x(t) - u if (low_bound == -Inf) low_bound &lt;- endsign(subcdf, -1) if (upp_bound == Inf) upp_bound &lt;- endsign(subcdf) return(uniroot(subcdf, lower=low_bound, upper=upp_bound, tol = 0.001220703)$root) } U &lt;- stats::runif(n) sapply(U, invcdf) Furthermore, we also introduce (not in the same simulations, obviously) a simpler version for this step, that computes this integral approximation, using the solution \\(\\int_{u}^{t}{V_s ds} = \\frac{1}{2} \\left( V_u + V_t \\right)\\). This solution is called drift interpolation [43] and its implementation is out of the box but we provide it anyway: int_v &lt;- dt * ((1/2) * v + (1/2) * vt) 3.4.3 Compute \\(\\int_0^t \\sqrt{V_s}dB_s\\) given \\(V_t\\), \\(V_0\\) and \\(\\int_0^t V_sds\\) From equation (??) we are now able to compute this integral: \\[\\begin{align} \\label{eq:ea3} \\int_{0}^{t}{\\sqrt{V_s} dB_s} = \\frac{V_t - V_0 - \\kappa \\theta t + \\kappa \\int_{0}^{t}{V_s ds}}{\\sigma} \\end{align}\\] The last step of the algorithm consists of computing the conditional distribution of \\(log S_t\\) based on the fact that the process for \\(V_t\\) is independent from \\(dB_t\\), and the distribution of \\(\\displaystyle \\int_0^t{\\sqrt{V_s} dB_s}\\) is normal with mean \\(0\\) and variance \\(\\displaystyle \\int_0^t{V_s ds}\\), given \\(V_t\\). \\[m(u,t) = \\log S_0 + \\left[ r t - \\frac{1}{2} \\int_{0}^{t}{V_s ds} + \\rho \\int_{0}^{t}{\\sqrt{V_s} d B_s} + \\sqrt{1 - \\rho^2} \\int_{0}^{t}{\\sqrt{V_s} dW_s}\\right]\\] and variance \\[\\sigma^2(0,t) = \\left( 1 - \\rho^2 \\right) \\int_0^t{V_s ds}\\] We generate the \\(S_t\\) sample using a standard normal random variable \\(Z\\) and set: \\[S_t = e^{m(0,t) + \\sigma (0,t) Z}\\] In R code, we have the following: int_vdw &lt;- (1/sigma) * (vt - v - k * theta * dt + k * int_v) m &lt;- log(S) + (r * (tau - t) - (1/2) * int_v + rho * int_vdw) std &lt;- sqrt((1 - rho^2)) * sqrt(int_v) S &lt;- exp(m + std * stats::rnorm(1)) 3.4.4 Limitations The biggest limitation this scheme presents is that the second step is computationally costly. It demands the inversion of the distribution function of \\(\\left( \\displaystyle \\int_0^t V_sds \\mid V_t, \\, V_0 \\right)\\) numerically. We must perform a root search of \\(F(x_i) - U = 0\\) testing for different \\(x_i\\). Notwithstanding, we do not know the cummulative form of \\(F(x_i)\\) distribution and have to perform our root finding strategy starting from the characteristic function, which contains two modified Bessel functions inside, in a structure that is rerun until a given tolerance \\(\\epsilon\\) is reached. Mathematically: \\(F(x_i) - U = \\epsilon\\). "],
["4-resultados.html", "Chapter 4 Results", " Chapter 4 Results We present here the results of all the implementations that were disclosed in the previous section. We perform numerical comparisons between all the methods, setting out differences accross number of simulations and timesteps. These simulations were performed as a way to validate our implementations, and thus do not have the intent to exhaust all kinds of possibilities or provide sharp comparisons. Heston [24] gives a closed form used as benchmark and as the ‘true’ option value and enabling the results to be exposed in terms of bias13 and RMSE (root mean square error).14 The simulaton experiments were performed on a notebook with an Intel(R) Core(TM) i7-4500U CPU @ 1.80GHz processor and 8GB of RAM running on a linux x86_64 based OS, Fedora 25. Codes were all written in R an run on version 3.4.1 “Single Candle” [2]. First of all, we chose a parametrizations based on what we saw in other works, made some adjustments, like reducing the options’ time to maturity due to the slower nature of R language and compared initial the results with the true option price given by the function callHestoncf belonging to the package NMOF [44]. Parameters can be seen in Table ?? below. As we said above, these parameter weren’t chosen at random, we took them from Table 2 of the Broadie-Kaya [31] paper, with two minor modifications performed. We reduced the time to maturity from 5 to 1 year and we reduced the \\(\\sigma\\) from 1 to 0.2 following the “rule” they acknowledge in their paper that \\(\\frac{2 \\kappa \\theta}{\\sigma} \\leq 1\\). The setup where the this inequality isn’t followed produces bigger biases, especially in Euler’s and Kahl-Jackel’s discretizations. In this situations, the EA often outperforms the Euler and KJ solutions in terms os RMSE. To perform our simulations, we first fixed a seed and saved the results in Table ??. Since the value given by the callHestoncf function with the parameters in Table ?? is 14.176, the method that best approached the “true” value was the modified (drift interpolated) exact algorithm with \\(100,000\\) simulations. Altough the Euler scheme gives the same result (14.16) as the modified EA with \\(10,000\\) simulations, it moves away from the closed form value when we run \\(100,000\\) simulations. Results are observable in Figure 4.1 also. The plot gives a good sense of possible biases associated with each method. It also show that the methods tend to converge quickly to the real result when the number of steps is increased. Figure 4.1: Comparison between models, 100 steps. To verify if in fact these methods present bias, we performed ten thousand simulations of ten thousand paths to the Euler, Kahl-Jackel (KJ) and drift interpolated exact algorithm (EA-DI) with 20 steps (\\(dt = 0.05\\)) first. We present these results in appendix C.4 because we present the more precise simulations we made, with 100 steps in Figure 4.2. Results seem to show a little bias, thus we present in Table ?? how precise models are. The red line presents the option’s true value. Figure 4.2: Comparison between models, 100 steps. The exact algorithm performs much slowlier than the other three we only performed 1000 simulations of one thousand paths each. That is why its RMSE is comparably bigger than the other three implementations. We are aware that, by construction the Broadie-Kaya version of the algorithm has no bias, but we tested the bias of our implementation (even though we didn’t put it in the Table ??) and the reason is twofold: again, we wanted to check if we correctly implemented the algorithm as a whole, but more importantly we wanted to check the outcomes of the adaptations we made (in terms of truncation, relative error and others) were playing into our model. The computed, even with far less simulations was far smaller than the other three methods, with only \\(-0.02\\). We understand that this result confirms that our adaptations did not change the nature of the model and with a greater number of simulations the bias would be zero as it should. We observe bias level in the three classical numerical models, the KJ model undervaluing in the same order (0.09) as the other two overvalue the option. In terms of standard deviation the models seem to be equivalent, with the KJ model presenting a slightly bigger standard deviation than the other two (0.24 against 0.21 and 0.22). According to the tests performed, we are confident that our models were implemented correctly. Defined as \\(\\mathbb{E} \\left[ \\hat{\\alpha} - \\alpha \\right]\\)↩ Defined as \\(\\sqrt{\\mathbb{E}((\\hat{\\theta}-\\theta)^2)}\\)↩ "],
["5-conclusao.html", "Chapter 5 Conclusion", " Chapter 5 Conclusion As we revisit what was presented through this thesis, we highlight that we were unable find codes of the heston’s exact algorithm in any shape or form. That means we searched if the algorithm implementation was available in any programming language, or at least a reference to where to find it and we couldn’t. Firstly, we provided a succint, but rigorous, theoretical framework. Since this topic is very embracing, we provided an abundant amount of literature not only to the core topics approached, but also to the marginal ones. The first topic introduced in our review of literature is the stochastic calculus subject. Following the basics, we present the Black-Scholes model since this is the basic model from which the stochastic volatility ones derive from. We show how the BS formula is computed, and its limitations. The next logical step are the stochastic volatility models since they directly attack and solve the biggest BS model limitation: fixed volatility. We present the CIR before the Heston model as the first concocts the latter. Lastly, approach the challenges brought by the adoption of numerical methods, to better understand if a method is theoretically good, how to perform a discretization, and why a supposedly good discretization scheme can fail, nevertheless. Thereafter, we thoroughly introduced the algorithms we were going to implement in R, and how we were going to do it. Four different algorithms were presented: the Euler scheme, the Kahl-Jackel algorithm, and two versions of the “exact algorithm”. The first trying to be as similar as possible to the one presented in Broadie-Kaya’s paper and the modified version bringing an approximation to the algorithm’s second step (the costlier step) as we replace the integral with a drift interpolation. We tested the implementations and presented our results in Chapter 4. We used a specification provided in Broadie-Kaya’s paper [31], Table 2 with two slight changes. We reduced the time to maturity from 5 to 1 year and we reduced the \\(\\sigma\\) from 1 to 0.2 following the rules he acknowledges and previously cited. We verified that, in fact, the EA algorithm does not present bias, but because of the nature of the algorithm and of the implementation in R, we doubt it can be used practically without finding a way around the algorithm’s second step. As we performed the EA’s implementation we faced some challenges we weren’t able to resolve yet. We couldn’t parallelize the root inversion, which was a big bottleneck for the algorithm that we will solve. Also, we only created methods for the European call and for four different schemes. We intend to widen the scope to embrace the put computation and introduce the binomial tree approach along with the method of moments and maximum likelihood. Finally, the package with all its functions is available at this 15 and in the near future on . If you feel it necessary to include an appendix, it goes here. --> https://github.com/fernote7/rnmethods↩ "],
["A-bsformula.html", "A Black-Scholes formula", " A Black-Scholes formula In this appendix, we start from the following Geometric Brownian Motion process: \\[\\begin{align*} dS_t &amp;= \\mu S_t dt + \\sigma S_t dB_t \\end{align*}\\] And we claim that the solution to this stochastic differential equation applying Itô’s calculus is the following formula: \\[\\begin{align*} S_T &amp;= S_0 \\times \\exp \\left( \\left( \\mu - \\frac{\\sigma^2}{2} \\right) T + \\sigma B_T \\right) \\end{align*}\\] "],
["B-bessel.html", "B Bessel Function", " B Bessel Function The modified Bessel function of the first kind can be described in the shape of a contour integral (below) and is plotted for three different \\(\\nu\\) in Figure B.1. \\[\\begin{align*} I_n(z)= \\frac{1}{2 \\pi i}\\displaystyle \\oint e^{(z/2)(t+1/t)}t^{-n-1}dt \\end{align*}\\] However, the Broadie-Kaya paper [31] presents it as a power series: \\[\\begin{align*} I_{\\nu}(z)= \\left(\\frac{1}{2}z\\right)^\\nu \\displaystyle \\sum_{k=0}^{\\infty}{\\frac{\\left(\\frac{1}{4}z^2\\right)^k}{\\left(k!\\Gamma \\left(\\nu+k+1\\right)\\right)}} \\end{align*}\\] With \\(\\Gamma(x)\\) is the gamma function and \\(\\nu\\) ia a complex number. Figure B.1: Modified Bessel Functions of the First Kind "],
["C-imps.html", "C Implementations C.1 Euler Scheme C.2 Kahl-Jackel Implementation C.3 Exact Algorithm C.4 Results with 20 steps", " C Implementations C.1 Euler Scheme hestoneuler &lt;- function(S, X, r, v, theta, rho, k, sigma, t = 0, dt = NULL, tau = 1, N){ if(is.null(dt)){ dt &lt;- (tau-t)/1000} sequencia &lt;- seq(t,tau,dt) ST &lt;- matrix(NA, length(sequencia), N) #transformar em matrix aux &lt;- NULL sqrt_dt &lt;- sqrt(dt) for(i in sequencia){ Zv &lt;- stats::rnorm(N) Zt &lt;- stats::rnorm(N) Zs &lt;- rho * Zv + (sqrt(1 - (rho^2)) * Zt) aux &lt;- v aux[v &lt; 0] &lt;- 0 sqrt_aux &lt;- sqrt(aux) S &lt;- S * (1 + r * dt + sqrt_aux * Zs * sqrt_dt) S[S &lt;= 0] = 0 v &lt;- v + k * dt * (theta - aux) + sigma * sqrt_aux * Zv * sqrt_dt ST[j,] &lt;- S } rm(aux, v, Zv, Zt, Zs, S, j) ST &lt;- as.matrix(ST, ncol=N) Result &lt;- ST[nrow(ST),] - X Result[Result &lt;= 0] = 0 call = mean(exp(-r*(tau-t))*Result) lista = list(&#39;call&#39; = call, &#39;Result&#39; = Result, &#39;Spot&#39; = ST) return(lista) } C.2 Kahl-Jackel Implementation Hestoncallkj &lt;- function(S, X, r, q, v, theta, rho, k, sigma, t = 0, dt = NULL, tau = 1, N){ if(is.null(dt)){ dt &lt;- (T-t)/1000} v &lt;- rep(v,N) theta&lt;- rep(theta,N) sequencia &lt;- seq(t,tau,dt) ST &lt;- matrix(NA, length(sequencia), N) #transformar em matrix S &lt;- log(S) for(i in seq(t,tau,dt)){ Zv &lt;- stats::rnorm(N) Zt &lt;- stats::rnorm(N) Zs &lt;- rho * Zv + sqrt(1 - rho^2) * Zt vt &lt;- (v + k * theta * dt + sigma * sqrt(v) * Zv * sqrt(dt) + (1/4) * sigma^2 * dt * ((Zv)^2 - 1))/(1 + k * dt) vt[vt &lt;= 0] &lt;- v[vt &lt;= 0] + k * dt * (theta[vt &lt;= 0] - v[vt &lt;= 0],0) + sigma * sqrt(v[vt &lt;= 0],0) * Zv[vt &lt;= 0] * sqrt(dt) v &lt;- vt v[v&lt;=0] &lt;- 0 vt[vt&lt;=0] &lt;- 0 S &lt;- S + (r - (v+vt)/4) * dt + rho * sqrt(v) * Zv * sqrt(dt) + (1/2) * (sqrt(v) + sqrt(vt)) * (Zs + rho * Zv) * sqrt(dt) + ((rho * sigma * dt)/2) * ((Zv)^2 - 1) S[S &lt;= 0] = 0 ST[j,] &lt;- S } ST &lt;- as.matrix(ST, ncol=N) Result &lt;- exp(ST[nrow(ST),]) - X Result[Result &lt;= 0] = 0 call = mean(exp(-r*tau)*Result) lista = list(&#39;call&#39; = call, &#39;Result&#39; = Result, &#39;Spot&#39; = ST) return(lista) } C.3 Exact Algorithm phi_heston &lt;- function(a, v0, v_t, d){ gamma_a &lt;- sqrt(k^2 - 2 * sigma^2 * 1i*a) gammadt &lt;- gamma_a * (tau-t) sqrtv0vt &lt;- sqrt(v0*v_t) delta &lt;- -k * (tau-t) part1 &lt;- (gamma_a * exp(-(gamma_a - k)/2 * (tau-t)) * (1 - exp(delta)))/ (k * (1- exp(- gammadt))) part2 &lt;- exp((v0+v_t)/(sigma^2) * ( (k * (1 + exp(delta)))/(1-exp(delta)) - (gamma_a * (1 + exp(- gammadt)))/(1-exp(- gammadt)))) part3 &lt;- Bessel::BesselI(z = ((4 * gamma_a * sqrtv0vt)/(sigma^2) * exp(- gammadt/2)/ (1 - exp(- gammadt))), nu = 0.5*d - 1) / Bessel::BesselI(z = ((4 * k * sqrtv0vt)/(sigma^2) * (exp(delta/2))/ (1-exp(delta))), nu = 0.5*d - 1) return (part1 * part2 * part3) } intv &lt;- function(n, cf, v_t){ integrand &lt;- function(x, phi = cf){ f2 &lt;- function(u){ Im(phi(u) * exp(-1i * u * x)) /u } return(f2) } ## integrate to &quot;cdf&quot; F_x &lt;- function (x) { y &lt;- 0.5 - 1/pi * integrate(integrand(x), lower= 0, upper= 1000, rel.tol = 0.001, stop.on.error = FALSE)$value return(y) } ## endsign endsign &lt;- function(f, sign = 1) { b &lt;- sign while (sign * f(b) &lt; 0) b &lt;- 10 * b return(b) } ## inversion low_bound = -Inf upp_bound = Inf invcdf &lt;- function(u) { subcdf &lt;- function(t) F_x(t) - u if (low_bound == -Inf) low_bound &lt;- endsign(subcdf, -1) if (upp_bound == Inf) upp_bound &lt;- endsign(subcdf) return(uniroot(subcdf, lower=low_bound, upper=upp_bound, tol = 0.001220703)$root) } U &lt;- stats::runif(n) sapply(U, invcdf) } hestonea_mod &lt;- function(S, X, r, v, theta, rho, k, sigma, t = 0, tau = 1){ d1 &lt;- (4 * k * theta)/(sigma)^2 c0 &lt;- (sigma^2 * (1 - exp(-k*tau)))/(4*k) dt &lt;- (tau-t) ST &lt;- NULL # sampling V lambda &lt;- (4*k*exp(-k*dt)*v)/(sigma^2 * (1-exp(-k*dt))) vt &lt;- c0 * stats::rchisq(n = 1, df = d1, ncp = lambda) # Sampling int{V} phi &lt;- function(a, v0=v, v_t=vt, d=d1){phi_heston(a, v0=v, v_t=vt, d=d1)} int_v &lt;- intv(1, cf = phi, v_t=vt) # OR if you perform the drift interpolation scheme: # int_v &lt;- dt * ((1/2) * v + (1/2) * vt) # Sampling int{v}dw int_vdw &lt;- (1/sigma) * (vt - v - k * theta * dt + k * int_v) # Sampling S if( int_v &gt;= 0){ m &lt;- log(S) + (r * (tau - t) - (1/2) * int_v + rho * int_vdw) std &lt;- sqrt((1 - rho^2)) * sqrt(int_v) S &lt;- exp(m + std * rnorm(1)) v &lt;- vt ST &lt;- S } else { v &lt;- vt ST &lt;- rbind(ST,NA)} Result &lt;- ST - X Result[Result &lt;= 0] = 0 call = exp(-r*tau)*Result lista = list(&#39;call&#39; = call, &#39;Result&#39; = Result, &#39;Spot&#39; = ST) return(lista) } C.4 Results with 20 steps Figure C.1: Comparison between models, 20 steps. "],
["D-references.html", "D References", " D References "]
]
